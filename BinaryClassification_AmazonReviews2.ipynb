{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Sentiment Classification of Amazon Food Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to serialization files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToBinClassDir = '/home/matt/w266_saved/binaryclassifier'\n",
    "pathToWordId      = '/home/matt/w266_saved/binaryclassifier/wordId.npy'\n",
    "pathToCheckpoint  = '/home/matt/w266_saved/binaryclassifier/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "numClasses = 2  # Binary classification\n",
    "hiddenSize = 50\n",
    "\n",
    "assert(batchSize % numClasses == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "wordsList = np.load(os.path.join(str(Path.home()), '.kaggle/wordvectors/pretrained_glove/wordsList.npy'))\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(str(Path.home()), '.kaggle/wordvectors/pretrained_glove/wordVectors.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors have dimension 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/matt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('~/.kaggle/datasets/snap/amazon-fine-food-reviews/Reviews.csv', encoding='utf8')\n",
    "review_df = review_df.drop(['ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the one and five star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df = review_df[review_df.Score == 1]\n",
    "one_df.reset_index(inplace=True)\n",
    "\n",
    "two_df = review_df[review_df.Score == 2]\n",
    "two_df.reset_index(inplace=True)\n",
    "\n",
    "four_df = review_df[review_df.Score == 4]\n",
    "four_df.reset_index(inplace=True)\n",
    "\n",
    "five_df = review_df[review_df.Score == 5]\n",
    "five_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit number of ratings for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_ratings = 116000\n",
    "rating_level_length = 29000\n",
    "assert(max_num_ratings <= 4*rating_level_length)\n",
    "\n",
    "one_df = one_df[0:rating_level_length]\n",
    "two_df = two_df[0:rating_level_length]\n",
    "four_df = four_df[0:rating_level_length]\n",
    "five_df = five_df[0:rating_level_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the size of the train, dev, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Train with 60%, , Dev: 10%, Test: 30%\n",
    "train_percent = 0.6\n",
    "dev_percent = 0.1\n",
    "test_percent = 0.3\n",
    "\n",
    "# Get indicies of the rows in the dataframe for training and testing\n",
    "train_lower_index = 0\n",
    "dev_lower_index   = math.floor(train_percent*max_num_ratings)\n",
    "test_lower_index  = math.floor( (train_percent+dev_percent)*max_num_ratings )\n",
    "\n",
    "train_size = dev_lower_index - train_lower_index\n",
    "dev_size   = test_lower_index - dev_lower_index\n",
    "test_size  = max_num_ratings - test_lower_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation, lowercase, and then tokenize the reviews.  The tokens need to be lowercase for the embedding lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return word_tokenize(re.sub(strip_special_chars, \" \", string.lower()))\n",
    "\n",
    "def custom_tokenize(string):\n",
    "    if not string:\n",
    "        string = ''\n",
    "    return word_tokenize(re.sub(strip_special_chars, \" \", string.lower()))\n",
    "\n",
    "one_df['Tokens'] = one_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "one_df['Summary_Tokens'] = one_df['Summary'].fillna(\"\").apply(custom_tokenize)\n",
    "two_df['Tokens'] = two_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "two_df['Summary_Tokens'] = two_df['Summary'].fillna(\"\").apply(custom_tokenize)\n",
    "four_df['Tokens'] = four_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "four_df['Summary_Tokens'] = four_df['Summary'].fillna(\"\").apply(custom_tokenize)\n",
    "five_df['Tokens'] = five_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "five_df['Summary_Tokens'] = five_df['Summary'].fillna(\"\").apply(custom_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the array of input sentences converted to word IDs. \n",
    "One extra integer to store the review ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSummaryLength = 11  # From EDA\n",
    "maxSeqLength     = maxSummaryLength + 267 # From EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = np.zeros((4*rating_level_length, maxSeqLength+2), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert words to word IDs and store in word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "word_id_file = Path(pathToWordId)\n",
    "\n",
    "if not word_id_file.exists():\n",
    "\n",
    "    sentence_index = 0\n",
    "\n",
    "    for df in [one_df, two_df, four_df, five_df]:\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "\n",
    "            # Store the review Id for identifying misclassified reviews in testing\n",
    "            word_index = 0\n",
    "            word_ids[sentence_index][word_index] = row['Id']\n",
    "            word_index = word_index + 1\n",
    "            word_ids[sentence_index][word_index] = row['Score']\n",
    "            word_index = word_index + 1\n",
    "\n",
    "            for word in row['Summary_Tokens']:\n",
    "\n",
    "                try:\n",
    "                    word_ids[sentence_index][word_index] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                    word_ids[sentence_index][word_index] = 399999 #Vector for unkown words\n",
    "\n",
    "                word_index = word_index + 1\n",
    "\n",
    "                if word_index == maxSummaryLength:\n",
    "                    break\n",
    "                    \n",
    "            for word in row['Tokens']:\n",
    "\n",
    "                try:\n",
    "                    word_ids[sentence_index][word_index] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                    word_ids[sentence_index][word_index] = 399999 #Vector for unkown words\n",
    "\n",
    "                word_index = word_index + 1\n",
    "\n",
    "                if word_index == maxSeqLength:\n",
    "                    break\n",
    "\n",
    "            sentence_index = sentence_index + 1\n",
    "\n",
    "    # Shuffle the word_ids matrix\n",
    "    np.random.shuffle(word_ids)\n",
    "    \n",
    "    # Save the word_ids matrix\n",
    "    binClassDir = Path(pathToBinClassDir)\n",
    "    \n",
    "    if not binClassDir.exists():\n",
    "        os.mkdir(pathToBinClassDir)\n",
    "    \n",
    "    np.save(pathToWordId, word_ids)\n",
    "else:\n",
    "    word_ids = np.load(pathToWordId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get the train and test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getRandomReviews(sectionOffset, sectionSize):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        num = randint(0, sectionSize-1) + sectionOffset\n",
    "        \n",
    "        if ( (word_ids[num, 1] == 1) or (word_ids[num, 1] == 2)  ):\n",
    "            labels.append([1, 0])\n",
    "        else: \n",
    "            labels.append([0, 1])\n",
    "        \n",
    "        arr[i] = word_ids[num, 2:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        \n",
    "    return arr, labels, ids\n",
    "\n",
    "\n",
    "reviewsDevIndex = 0\n",
    "reviewsTestIndex = 0\n",
    "\n",
    "def getOrderedDevReviews(sectionOffset, sectionSize):\n",
    "    global reviewsDevIndex\n",
    "    \n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    finished = False\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        if reviewsDevIndex >= sectionSize:\n",
    "            finished = True\n",
    "            break;\n",
    "\n",
    "        num = reviewsDevIndex + sectionOffset\n",
    "        \n",
    "        if (word_ids[num, 1] == 1) or (word_ids[num, 1] == 2):\n",
    "            labels.append([1, 0])\n",
    "        else: \n",
    "            labels.append([0, 1])\n",
    "        \n",
    "        arr[i] = word_ids[num, 2:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        reviewsDevIndex += 1\n",
    "        \n",
    "    if reviewsDevIndex == sectionSize:\n",
    "        finished = True\n",
    "        \n",
    "    return arr, labels, ids, finished\n",
    "\n",
    "def getOrderedTestReviews(sectionOffset, sectionSize):\n",
    "    global reviewsTestIndex\n",
    "    \n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    finished = False\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        if reviewsTestIndex >= sectionSize:\n",
    "            finished = True\n",
    "            break;\n",
    "\n",
    "        num = reviewsTestIndex + sectionOffset\n",
    "        \n",
    "        if (word_ids[num, 1] == 1) or (word_ids[num, 1] == 2):\n",
    "            labels.append([1, 0])\n",
    "        else: \n",
    "            labels.append([0, 1])\n",
    "        \n",
    "        arr[i] = word_ids[num, 2:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        reviewsTestIndex += 1\n",
    "        \n",
    "    if reviewsTestIndex == sectionSize:\n",
    "        finished = True\n",
    "        \n",
    "    return arr, labels, ids, finished\n",
    "\n",
    "def resetDevTestIndicies():\n",
    "    global reviewsDevIndex\n",
    "    global reviewsTestIndex\n",
    "\n",
    "    reviewsDevIndex = 0\n",
    "    reviewsTestIndex = 0\n",
    "\n",
    "def getTrainBatch():\n",
    "    return getRandomReviews(train_lower_index, train_size)\n",
    "\n",
    "def getDevBatch():\n",
    "    return getOrderedDevReviews(dev_lower_index, dev_size)\n",
    "\n",
    "def getTestBatch():\n",
    "    return getOrderedTestReviews(test_lower_index, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations = 200000\n",
    "learning_rate =  0.001\n",
    "dropout_keep_prob = 0.75\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "        self.input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "        self.prediction = None\n",
    "        self.accuracy = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    def MakeFancyRNNCell(self, H, keep_prob, num_layers=1):\n",
    "        cells = []\n",
    "        for _ in range(num_layers):\n",
    "          cell = tf.nn.rnn_cell.BasicLSTMCell(H, forget_bias=0.0)\n",
    "          cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "              cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "          cells.append(cell)\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "    def CreateGraph(self):\n",
    "        data = tf.Variable(tf.zeros([batchSize, maxSeqLength, embedding_dimension]), dtype=tf.float32)\n",
    "        data = tf.nn.embedding_lookup(wordVectors, self.input_data)\n",
    "\n",
    "        lstmCell = self.MakeFancyRNNCell(hiddenSize, dropout_keep_prob, num_layers)\n",
    "        initial_h_ = lstmCell.zero_state(batchSize, dtype=tf.float32)\n",
    "        \n",
    "        rnn_out, _ = tf.nn.dynamic_rnn(lstmCell,\n",
    "                                       data,\n",
    "                                       initial_state=initial_h_,\n",
    "                                       dtype=tf.float32)\n",
    "\n",
    "        W_out = tf.Variable(tf.random_uniform([hiddenSize, numClasses], minval=-1.0, maxval=1.0, dtype=tf.float32), dtype=tf.float32)\n",
    "        b_out = tf.Variable(tf.zeros([numClasses,], dtype=tf.float32), dtype=tf.float32)\n",
    "\n",
    "        # Get the output of the last RNN cell\n",
    "        rnn_out = tf.transpose(rnn_out, [1, 0, 2])\n",
    "        last_cell_out = tf.gather(rnn_out, int(rnn_out.get_shape()[0]) - 1)\n",
    "\n",
    "        # Calculate logits\n",
    "        logits = (tf.matmul(last_cell_out, W_out) + b_out)\n",
    "\n",
    "        # Calculate prediction and accuracy\n",
    "        self.prediction = tf.argmax(logits,1)\n",
    "        correctPred = tf.equal(self.prediction, tf.argmax(self.labels,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.labels))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def TrainModel(session, logdir, graph):\n",
    "    \n",
    "    # Open the writer\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    \n",
    "    tf.summary.scalar('Train_Loss', graph.loss)\n",
    "    tf.summary.scalar('Train_Accuracy', graph.accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "        \n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    i = 0  # Must stay outside the loops\n",
    "    \n",
    "    for epoch in range(train_iterations):\n",
    "        \n",
    "        # Next Batch of reviews\n",
    "        nextBatch, nextBatchLabels, reviewIds = getTrainBatch()\n",
    "\n",
    "        feed_dict_ = {\n",
    "            graph.input_data: nextBatch,\n",
    "            graph.labels: nextBatchLabels\n",
    "        }\n",
    "\n",
    "        loss_, _ = session.run([graph.loss, graph.optimizer], feed_dict=feed_dict_)\n",
    "\n",
    "        # Write summary to Tensorboard\n",
    "        if (i % 10 == 0):\n",
    "            summary = session.run(merged, {graph.input_data: nextBatch, graph.labels: nextBatchLabels})\n",
    "            writer.add_summary(summary, i)\n",
    "\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"Loss is: \", loss_, \", \", (datetime.datetime.now() - start_time).seconds, \" seconds, iteration: \", i)\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the Mispredicted_AmazonBinaryClassification.csv file for error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModelDev(session, logdir, graph):\n",
    "\n",
    "    # Support for saving mispredicted reviews\n",
    "    csv = open('Mispredicted_AmazonBinaryClassification.csv', 'w')\n",
    "    csv.write(\"Id\\n\")\n",
    "    \n",
    "    # Tensorboard support\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    tf.summary.scalar('Dev_Loss', graph.loss)\n",
    "    tf.summary.scalar('Dev_Accuracy', graph.accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    start_time = datetime.datetime.now()\n",
    "    i = 0  # Must stay outside the loops\n",
    "    \n",
    "    accuracy_measurements = []\n",
    "    loss_measurements = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "    \n",
    "        nextBatch, nextBatchLabels, reviewIds, finished = getDevBatch()\n",
    "\n",
    "        # For ease of implementation, just skip partially filled batches\n",
    "        if not finished:\n",
    "            feed_dict = {\n",
    "                graph.input_data: nextBatch,\n",
    "                graph.labels: nextBatchLabels\n",
    "            }\n",
    "\n",
    "            accuracy_, loss_ = sess.run([graph.accuracy, graph.loss], feed_dict)\n",
    "            \n",
    "            # Write summary to Tensorboard\n",
    "            if (i % 10 == 0):\n",
    "                summary = session.run(merged, {graph.input_data: nextBatch, graph.labels: nextBatchLabels})\n",
    "                writer.add_summary(summary, i)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            accuracy_measurements.append(accuracy_)\n",
    "            loss_measurements.append(loss_)\n",
    "            \n",
    "            # Write out mispredictions (review IDs) to a .csv file\n",
    "            if accuracy_ < 1.0:\n",
    "                \n",
    "                predictions_ = sess.run(graph.prediction, feed_dict)\n",
    "\n",
    "                for index in range(len(predictions_)):\n",
    "\n",
    "                    if predictions_[index] != np.argmax(nextBatchLabels[index]):\n",
    "                        csv.write(str(int(reviewIds[index])) + \"\\n\")\n",
    "\n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    \n",
    "    print('Testing (Dev) Results:')\n",
    "    print('The average accuracy is: ', np.mean(accuracy_measurements))\n",
    "    print('The average loss is: ', np.mean(loss_measurements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModelTest(session, logdir, graph):\n",
    "    \n",
    "    # Tensorboard support\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    tf.summary.scalar('Test_Loss', graph.loss)\n",
    "    tf.summary.scalar('Test_Accuracy', graph.accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    start_time = datetime.datetime.now()\n",
    "    i = 0  # Must stay outside the loops\n",
    "    \n",
    "    accuracy_measurements = []\n",
    "    loss_measurements = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "    \n",
    "        nextBatch, nextBatchLabels, reviewIds, finished = getTestBatch()\n",
    "\n",
    "        # For ease of implementation, just skip partially filled batches\n",
    "        if not finished:\n",
    "            feed_dict = {\n",
    "                graph.input_data: nextBatch,\n",
    "                graph.labels: nextBatchLabels\n",
    "            }\n",
    "\n",
    "            accuracy_, loss_ = sess.run([graph.accuracy, graph.loss], feed_dict)\n",
    "\n",
    "            # Write summary to Tensorboard\n",
    "            if (i % 10 == 0):\n",
    "                summary = session.run(merged, {graph.input_data: nextBatch, graph.labels: nextBatchLabels})\n",
    "                writer.add_summary(summary, i)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            accuracy_measurements.append(accuracy_)\n",
    "            loss_measurements.append(loss_)\n",
    "                    \n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    \n",
    "    print('Testing (Test) Results:')\n",
    "    print('The average accuracy is: ', np.mean(accuracy_measurements))\n",
    "    print('The average loss is: ', np.mean(loss_measurements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prior to training or testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following:\n",
    "tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.6931472 ,  0  seconds, iteration:  0\n",
      "Loss is:  0.6828273 ,  94  seconds, iteration:  1000\n",
      "Loss is:  0.702293 ,  178  seconds, iteration:  2000\n",
      "Loss is:  0.71521074 ,  245  seconds, iteration:  3000\n",
      "Loss is:  0.69490576 ,  313  seconds, iteration:  4000\n",
      "Loss is:  0.67615104 ,  380  seconds, iteration:  5000\n",
      "Loss is:  0.68719053 ,  448  seconds, iteration:  6000\n",
      "Loss is:  0.6983556 ,  515  seconds, iteration:  7000\n",
      "Loss is:  0.6970288 ,  583  seconds, iteration:  8000\n",
      "Loss is:  0.6914541 ,  651  seconds, iteration:  9000\n",
      "Loss is:  0.6644372 ,  718  seconds, iteration:  10000\n",
      "Loss is:  0.6964162 ,  786  seconds, iteration:  11000\n",
      "Loss is:  0.6950415 ,  853  seconds, iteration:  12000\n",
      "Loss is:  0.69777656 ,  921  seconds, iteration:  13000\n",
      "Loss is:  0.6818164 ,  988  seconds, iteration:  14000\n",
      "Loss is:  0.6848648 ,  1056  seconds, iteration:  15000\n",
      "Loss is:  0.6888134 ,  1123  seconds, iteration:  16000\n",
      "Loss is:  0.7102415 ,  1190  seconds, iteration:  17000\n",
      "Loss is:  0.69239634 ,  1286  seconds, iteration:  18000\n",
      "Loss is:  0.69688725 ,  1398  seconds, iteration:  19000\n",
      "Loss is:  0.6179304 ,  1508  seconds, iteration:  20000\n",
      "Loss is:  0.7332886 ,  1619  seconds, iteration:  21000\n",
      "Loss is:  0.69463724 ,  1731  seconds, iteration:  22000\n",
      "Loss is:  0.67314345 ,  1842  seconds, iteration:  23000\n",
      "Loss is:  0.6707947 ,  1927  seconds, iteration:  24000\n",
      "Loss is:  0.66818094 ,  1994  seconds, iteration:  25000\n",
      "Loss is:  0.69714063 ,  2062  seconds, iteration:  26000\n",
      "Loss is:  0.6690814 ,  2129  seconds, iteration:  27000\n",
      "Loss is:  0.77564234 ,  2196  seconds, iteration:  28000\n",
      "Loss is:  0.75898725 ,  2269  seconds, iteration:  29000\n",
      "Loss is:  0.68643427 ,  2337  seconds, iteration:  30000\n",
      "Loss is:  0.64306694 ,  2404  seconds, iteration:  31000\n",
      "Loss is:  0.6477626 ,  2472  seconds, iteration:  32000\n",
      "Loss is:  0.72296715 ,  2539  seconds, iteration:  33000\n",
      "Loss is:  0.6959531 ,  2607  seconds, iteration:  34000\n",
      "Loss is:  0.7418897 ,  2675  seconds, iteration:  35000\n",
      "Loss is:  0.62359846 ,  2744  seconds, iteration:  36000\n",
      "Loss is:  0.6160612 ,  2811  seconds, iteration:  37000\n",
      "Loss is:  0.67803496 ,  2879  seconds, iteration:  38000\n",
      "Loss is:  0.7133859 ,  2947  seconds, iteration:  39000\n",
      "Loss is:  0.6893551 ,  3014  seconds, iteration:  40000\n",
      "Loss is:  0.6840501 ,  3086  seconds, iteration:  41000\n",
      "Loss is:  0.6799852 ,  3154  seconds, iteration:  42000\n",
      "Loss is:  0.6959341 ,  3222  seconds, iteration:  43000\n",
      "Loss is:  0.6999669 ,  3289  seconds, iteration:  44000\n",
      "Loss is:  0.6840388 ,  3357  seconds, iteration:  45000\n",
      "Loss is:  0.692012 ,  3426  seconds, iteration:  46000\n",
      "Loss is:  0.6569565 ,  3496  seconds, iteration:  47000\n",
      "Loss is:  0.69721395 ,  3564  seconds, iteration:  48000\n",
      "Loss is:  0.6924646 ,  3632  seconds, iteration:  49000\n",
      "Loss is:  0.67963153 ,  3700  seconds, iteration:  50000\n",
      "Loss is:  0.69360787 ,  3768  seconds, iteration:  51000\n",
      "Loss is:  0.6449558 ,  3837  seconds, iteration:  52000\n",
      "Loss is:  0.6685607 ,  3905  seconds, iteration:  53000\n",
      "Loss is:  0.6800711 ,  3972  seconds, iteration:  54000\n",
      "Loss is:  0.5155721 ,  4040  seconds, iteration:  55000\n",
      "Loss is:  0.5670009 ,  4107  seconds, iteration:  56000\n",
      "Loss is:  0.6677549 ,  4176  seconds, iteration:  57000\n",
      "Loss is:  0.67680216 ,  4244  seconds, iteration:  58000\n",
      "Loss is:  0.6847283 ,  4311  seconds, iteration:  59000\n",
      "Loss is:  0.6781481 ,  4379  seconds, iteration:  60000\n",
      "Loss is:  0.7284446 ,  4447  seconds, iteration:  61000\n",
      "Loss is:  0.69230413 ,  4515  seconds, iteration:  62000\n",
      "Loss is:  0.64952046 ,  4583  seconds, iteration:  63000\n",
      "Loss is:  0.6577098 ,  4651  seconds, iteration:  64000\n",
      "Loss is:  0.64636487 ,  4719  seconds, iteration:  65000\n",
      "Loss is:  0.6599208 ,  4787  seconds, iteration:  66000\n",
      "Loss is:  0.68260115 ,  4856  seconds, iteration:  67000\n",
      "Loss is:  0.66646796 ,  4928  seconds, iteration:  68000\n",
      "Loss is:  0.58184844 ,  4995  seconds, iteration:  69000\n",
      "Loss is:  0.6711035 ,  5064  seconds, iteration:  70000\n",
      "Loss is:  0.6486208 ,  5133  seconds, iteration:  71000\n",
      "Loss is:  0.67895406 ,  5203  seconds, iteration:  72000\n",
      "Loss is:  0.6896283 ,  5273  seconds, iteration:  73000\n",
      "Loss is:  0.6445588 ,  5343  seconds, iteration:  74000\n",
      "Loss is:  0.53769654 ,  5413  seconds, iteration:  75000\n",
      "Loss is:  0.44138265 ,  5483  seconds, iteration:  76000\n",
      "Loss is:  0.39090762 ,  5553  seconds, iteration:  77000\n",
      "Loss is:  0.33974364 ,  5623  seconds, iteration:  78000\n",
      "Loss is:  0.5356129 ,  5693  seconds, iteration:  79000\n",
      "Loss is:  0.2382534 ,  5763  seconds, iteration:  80000\n",
      "Loss is:  0.45236087 ,  5846  seconds, iteration:  81000\n",
      "Loss is:  0.26917556 ,  5916  seconds, iteration:  82000\n",
      "Loss is:  0.45092544 ,  5986  seconds, iteration:  83000\n",
      "Loss is:  0.22763056 ,  6056  seconds, iteration:  84000\n",
      "Loss is:  0.19593526 ,  6126  seconds, iteration:  85000\n",
      "Loss is:  0.61709374 ,  6198  seconds, iteration:  86000\n",
      "Loss is:  0.17228128 ,  6268  seconds, iteration:  87000\n",
      "Loss is:  0.26769674 ,  6338  seconds, iteration:  88000\n",
      "Loss is:  0.41279292 ,  6408  seconds, iteration:  89000\n",
      "Loss is:  0.25143036 ,  6478  seconds, iteration:  90000\n",
      "Loss is:  0.30813012 ,  6548  seconds, iteration:  91000\n",
      "Loss is:  0.21734619 ,  6618  seconds, iteration:  92000\n",
      "Loss is:  0.35750687 ,  6688  seconds, iteration:  93000\n",
      "Loss is:  0.5026307 ,  6758  seconds, iteration:  94000\n",
      "Loss is:  0.32339096 ,  6828  seconds, iteration:  95000\n",
      "Loss is:  0.19232266 ,  6898  seconds, iteration:  96000\n",
      "Loss is:  0.24403279 ,  6968  seconds, iteration:  97000\n",
      "Loss is:  0.3598629 ,  7038  seconds, iteration:  98000\n",
      "Loss is:  0.14877819 ,  7131  seconds, iteration:  99000\n",
      "Loss is:  0.52314013 ,  7226  seconds, iteration:  100000\n",
      "Loss is:  0.10488806 ,  7322  seconds, iteration:  101000\n",
      "Loss is:  0.38457608 ,  7418  seconds, iteration:  102000\n",
      "Loss is:  0.16455476 ,  7513  seconds, iteration:  103000\n",
      "Loss is:  0.27865958 ,  7609  seconds, iteration:  104000\n",
      "Loss is:  0.2158575 ,  7704  seconds, iteration:  105000\n",
      "Loss is:  0.3700349 ,  7800  seconds, iteration:  106000\n",
      "Loss is:  0.16523035 ,  7896  seconds, iteration:  107000\n",
      "Loss is:  0.12055167 ,  7993  seconds, iteration:  108000\n",
      "Loss is:  0.10734423 ,  8089  seconds, iteration:  109000\n",
      "Loss is:  0.07380128 ,  8185  seconds, iteration:  110000\n",
      "Loss is:  0.1796819 ,  8281  seconds, iteration:  111000\n",
      "Loss is:  0.5110973 ,  8377  seconds, iteration:  112000\n",
      "Loss is:  0.24318975 ,  8473  seconds, iteration:  113000\n",
      "Loss is:  0.29527596 ,  8569  seconds, iteration:  114000\n",
      "Loss is:  0.5190154 ,  8665  seconds, iteration:  115000\n",
      "Loss is:  0.14047201 ,  8761  seconds, iteration:  116000\n",
      "Loss is:  0.18597285 ,  8857  seconds, iteration:  117000\n",
      "Loss is:  0.070106536 ,  8952  seconds, iteration:  118000\n",
      "Loss is:  0.24689473 ,  9048  seconds, iteration:  119000\n",
      "Loss is:  0.26303983 ,  9143  seconds, iteration:  120000\n",
      "Loss is:  0.29899886 ,  9239  seconds, iteration:  121000\n",
      "Loss is:  0.58103997 ,  9334  seconds, iteration:  122000\n",
      "Loss is:  0.09253921 ,  9431  seconds, iteration:  123000\n",
      "Loss is:  0.19583146 ,  9526  seconds, iteration:  124000\n",
      "Loss is:  0.11925397 ,  9622  seconds, iteration:  125000\n",
      "Loss is:  0.2779636 ,  9717  seconds, iteration:  126000\n",
      "Loss is:  0.21555085 ,  9812  seconds, iteration:  127000\n",
      "Loss is:  0.23610242 ,  9907  seconds, iteration:  128000\n",
      "Loss is:  0.2839121 ,  10002  seconds, iteration:  129000\n",
      "Loss is:  0.16059543 ,  10096  seconds, iteration:  130000\n",
      "Loss is:  0.103672706 ,  10190  seconds, iteration:  131000\n",
      "Loss is:  0.18648745 ,  10285  seconds, iteration:  132000\n",
      "Loss is:  0.13672061 ,  10379  seconds, iteration:  133000\n",
      "Loss is:  0.15250874 ,  10474  seconds, iteration:  134000\n",
      "Loss is:  0.13786262 ,  10569  seconds, iteration:  135000\n",
      "Loss is:  0.07302645 ,  10664  seconds, iteration:  136000\n",
      "Loss is:  0.24890392 ,  10760  seconds, iteration:  137000\n",
      "Loss is:  0.07520742 ,  10855  seconds, iteration:  138000\n",
      "Loss is:  0.112151004 ,  10951  seconds, iteration:  139000\n",
      "Loss is:  0.12343279 ,  11047  seconds, iteration:  140000\n",
      "Loss is:  0.18406968 ,  11144  seconds, iteration:  141000\n",
      "Loss is:  0.16678591 ,  11240  seconds, iteration:  142000\n",
      "Loss is:  0.14327449 ,  11335  seconds, iteration:  143000\n",
      "Loss is:  0.4902451 ,  11429  seconds, iteration:  144000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.23297672 ,  11523  seconds, iteration:  145000\n",
      "Loss is:  0.14541161 ,  11619  seconds, iteration:  146000\n",
      "Loss is:  0.2659092 ,  11713  seconds, iteration:  147000\n",
      "Loss is:  0.14263712 ,  11809  seconds, iteration:  148000\n",
      "Loss is:  0.22243015 ,  11904  seconds, iteration:  149000\n",
      "Loss is:  0.105272435 ,  12001  seconds, iteration:  150000\n",
      "Loss is:  0.13330153 ,  12097  seconds, iteration:  151000\n",
      "Loss is:  0.26485875 ,  12194  seconds, iteration:  152000\n",
      "Loss is:  0.19790058 ,  12290  seconds, iteration:  153000\n",
      "Loss is:  0.23897326 ,  12386  seconds, iteration:  154000\n",
      "Loss is:  0.26053932 ,  12482  seconds, iteration:  155000\n",
      "Loss is:  0.3214817 ,  12578  seconds, iteration:  156000\n",
      "Loss is:  0.11014327 ,  12674  seconds, iteration:  157000\n",
      "Loss is:  0.14146888 ,  12769  seconds, iteration:  158000\n",
      "Loss is:  0.047383357 ,  12864  seconds, iteration:  159000\n",
      "Loss is:  0.2725958 ,  12959  seconds, iteration:  160000\n",
      "Loss is:  0.07970559 ,  13055  seconds, iteration:  161000\n",
      "Loss is:  0.15479161 ,  13156  seconds, iteration:  162000\n",
      "Loss is:  0.16492324 ,  13255  seconds, iteration:  163000\n",
      "Loss is:  0.29885772 ,  13351  seconds, iteration:  164000\n",
      "Loss is:  0.34431562 ,  13447  seconds, iteration:  165000\n",
      "Loss is:  0.115553565 ,  13543  seconds, iteration:  166000\n",
      "Loss is:  0.11210779 ,  13639  seconds, iteration:  167000\n",
      "Loss is:  0.2394867 ,  13735  seconds, iteration:  168000\n",
      "Loss is:  0.12851745 ,  13831  seconds, iteration:  169000\n",
      "Loss is:  0.1825339 ,  13927  seconds, iteration:  170000\n",
      "Loss is:  0.06017618 ,  14023  seconds, iteration:  171000\n",
      "Loss is:  0.072163306 ,  14119  seconds, iteration:  172000\n",
      "Loss is:  0.3051928 ,  14215  seconds, iteration:  173000\n",
      "Loss is:  0.21272969 ,  14311  seconds, iteration:  174000\n",
      "Loss is:  0.14920224 ,  14406  seconds, iteration:  175000\n",
      "Loss is:  0.27632454 ,  14502  seconds, iteration:  176000\n",
      "Loss is:  0.0934237 ,  14597  seconds, iteration:  177000\n",
      "Loss is:  0.08898019 ,  14692  seconds, iteration:  178000\n",
      "Loss is:  0.24873178 ,  14788  seconds, iteration:  179000\n",
      "Loss is:  0.096726604 ,  14883  seconds, iteration:  180000\n",
      "Loss is:  0.14327604 ,  14979  seconds, iteration:  181000\n",
      "Loss is:  0.35739073 ,  15075  seconds, iteration:  182000\n",
      "Loss is:  0.14570485 ,  15170  seconds, iteration:  183000\n",
      "Loss is:  0.4041946 ,  15266  seconds, iteration:  184000\n",
      "Loss is:  0.12839732 ,  15362  seconds, iteration:  185000\n",
      "Loss is:  0.37358806 ,  15458  seconds, iteration:  186000\n",
      "Loss is:  0.20421119 ,  15554  seconds, iteration:  187000\n",
      "Loss is:  0.23683381 ,  15650  seconds, iteration:  188000\n",
      "Loss is:  0.04798327 ,  15746  seconds, iteration:  189000\n",
      "Loss is:  0.11273607 ,  15842  seconds, iteration:  190000\n",
      "Loss is:  0.16248906 ,  15937  seconds, iteration:  191000\n",
      "Loss is:  0.1444552 ,  16034  seconds, iteration:  192000\n",
      "Loss is:  0.4424398 ,  16129  seconds, iteration:  193000\n",
      "Loss is:  0.18692012 ,  16225  seconds, iteration:  194000\n",
      "Loss is:  0.49774337 ,  16319  seconds, iteration:  195000\n",
      "Loss is:  0.11429582 ,  16415  seconds, iteration:  196000\n",
      "Loss is:  0.3006645 ,  16510  seconds, iteration:  197000\n",
      "Loss is:  0.06104411 ,  16606  seconds, iteration:  198000\n",
      "Loss is:  0.1160504 ,  16702  seconds, iteration:  199000\n",
      "Loss is:  0.18398046 ,  16798  seconds, iteration:  200000\n",
      "Loss is:  0.16433492 ,  16894  seconds, iteration:  201000\n",
      "Loss is:  0.32509884 ,  16990  seconds, iteration:  202000\n",
      "Loss is:  0.14151369 ,  17086  seconds, iteration:  203000\n",
      "Loss is:  0.30651832 ,  17182  seconds, iteration:  204000\n",
      "Loss is:  0.16953129 ,  17278  seconds, iteration:  205000\n",
      "Loss is:  0.075776555 ,  17374  seconds, iteration:  206000\n",
      "Loss is:  0.18317075 ,  17471  seconds, iteration:  207000\n",
      "Loss is:  0.0937371 ,  17567  seconds, iteration:  208000\n",
      "Loss is:  0.09802198 ,  17663  seconds, iteration:  209000\n",
      "Loss is:  0.20048921 ,  17759  seconds, iteration:  210000\n",
      "Loss is:  0.10501712 ,  17856  seconds, iteration:  211000\n",
      "Loss is:  0.07317874 ,  17952  seconds, iteration:  212000\n",
      "Loss is:  0.085092045 ,  18047  seconds, iteration:  213000\n",
      "Loss is:  0.111342765 ,  18143  seconds, iteration:  214000\n",
      "Loss is:  0.1492935 ,  18239  seconds, iteration:  215000\n",
      "Loss is:  0.20034747 ,  18335  seconds, iteration:  216000\n",
      "Loss is:  0.22575967 ,  18431  seconds, iteration:  217000\n",
      "Loss is:  0.1306652 ,  18527  seconds, iteration:  218000\n",
      "Loss is:  0.2299607 ,  18623  seconds, iteration:  219000\n",
      "Loss is:  0.17269625 ,  18720  seconds, iteration:  220000\n",
      "Loss is:  0.09849835 ,  18816  seconds, iteration:  221000\n",
      "Loss is:  0.070611425 ,  18912  seconds, iteration:  222000\n",
      "Loss is:  0.17006022 ,  19008  seconds, iteration:  223000\n",
      "Loss is:  0.10475135 ,  19104  seconds, iteration:  224000\n",
      "Loss is:  0.055924803 ,  19198  seconds, iteration:  225000\n",
      "Loss is:  0.09570435 ,  19293  seconds, iteration:  226000\n",
      "Loss is:  0.113108896 ,  19387  seconds, iteration:  227000\n",
      "Loss is:  0.114983045 ,  19482  seconds, iteration:  228000\n",
      "Loss is:  0.1463828 ,  19576  seconds, iteration:  229000\n",
      "Loss is:  0.20854028 ,  19671  seconds, iteration:  230000\n",
      "Loss is:  0.07996947 ,  19767  seconds, iteration:  231000\n",
      "Loss is:  0.08962724 ,  19863  seconds, iteration:  232000\n",
      "Loss is:  0.050966527 ,  19959  seconds, iteration:  233000\n",
      "Loss is:  0.22783214 ,  20055  seconds, iteration:  234000\n",
      "Loss is:  0.17465967 ,  20151  seconds, iteration:  235000\n",
      "Loss is:  0.088514544 ,  20247  seconds, iteration:  236000\n",
      "Loss is:  0.13089179 ,  20343  seconds, iteration:  237000\n",
      "Loss is:  0.24996318 ,  20439  seconds, iteration:  238000\n",
      "Loss is:  0.22501004 ,  20535  seconds, iteration:  239000\n",
      "Loss is:  0.13409108 ,  20631  seconds, iteration:  240000\n",
      "Loss is:  0.25286797 ,  20727  seconds, iteration:  241000\n",
      "Loss is:  0.11341888 ,  20823  seconds, iteration:  242000\n",
      "Loss is:  0.21700299 ,  20919  seconds, iteration:  243000\n",
      "Loss is:  0.13999812 ,  21015  seconds, iteration:  244000\n",
      "Loss is:  0.2765006 ,  21110  seconds, iteration:  245000\n",
      "Loss is:  0.07468276 ,  21206  seconds, iteration:  246000\n",
      "Loss is:  0.28046206 ,  21302  seconds, iteration:  247000\n",
      "Loss is:  0.17799251 ,  21398  seconds, iteration:  248000\n",
      "Loss is:  0.053761587 ,  21494  seconds, iteration:  249000\n",
      "Loss is:  0.5507166 ,  21590  seconds, iteration:  250000\n",
      "Loss is:  0.08072108 ,  21686  seconds, iteration:  251000\n",
      "Loss is:  0.3263843 ,  21782  seconds, iteration:  252000\n",
      "Loss is:  0.08587193 ,  21878  seconds, iteration:  253000\n",
      "Loss is:  0.09605894 ,  21974  seconds, iteration:  254000\n",
      "Loss is:  0.0431477 ,  22070  seconds, iteration:  255000\n",
      "Loss is:  0.08354068 ,  22166  seconds, iteration:  256000\n",
      "Loss is:  0.10734942 ,  22261  seconds, iteration:  257000\n",
      "Loss is:  0.1681834 ,  22357  seconds, iteration:  258000\n",
      "Loss is:  0.2226203 ,  22453  seconds, iteration:  259000\n",
      "Loss is:  0.08503293 ,  22549  seconds, iteration:  260000\n",
      "Loss is:  0.117601484 ,  22645  seconds, iteration:  261000\n",
      "Loss is:  0.30749062 ,  22741  seconds, iteration:  262000\n",
      "Loss is:  0.083099715 ,  22837  seconds, iteration:  263000\n",
      "Loss is:  0.12206322 ,  22933  seconds, iteration:  264000\n",
      "Loss is:  0.29723755 ,  23028  seconds, iteration:  265000\n",
      "Loss is:  0.25294924 ,  23123  seconds, iteration:  266000\n",
      "Loss is:  0.04443549 ,  23218  seconds, iteration:  267000\n",
      "Loss is:  0.22779603 ,  23314  seconds, iteration:  268000\n",
      "Loss is:  0.12619598 ,  23410  seconds, iteration:  269000\n",
      "Loss is:  0.1716491 ,  23506  seconds, iteration:  270000\n",
      "Loss is:  0.18584669 ,  23601  seconds, iteration:  271000\n",
      "Loss is:  0.14636259 ,  23698  seconds, iteration:  272000\n",
      "Loss is:  0.18381824 ,  23794  seconds, iteration:  273000\n",
      "Loss is:  0.165693 ,  23890  seconds, iteration:  274000\n",
      "Loss is:  0.2752321 ,  23986  seconds, iteration:  275000\n",
      "Loss is:  0.10265765 ,  24082  seconds, iteration:  276000\n",
      "Loss is:  0.08100467 ,  24178  seconds, iteration:  277000\n",
      "Loss is:  0.39079678 ,  24274  seconds, iteration:  278000\n",
      "Loss is:  0.19726558 ,  24370  seconds, iteration:  279000\n",
      "Loss is:  0.16042908 ,  24466  seconds, iteration:  280000\n",
      "Loss is:  0.09782737 ,  24563  seconds, iteration:  281000\n",
      "Loss is:  0.124152265 ,  24658  seconds, iteration:  282000\n",
      "Loss is:  0.13883062 ,  24755  seconds, iteration:  283000\n",
      "Loss is:  0.24714954 ,  24850  seconds, iteration:  284000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.14576487 ,  24945  seconds, iteration:  285000\n",
      "Loss is:  0.14820077 ,  25041  seconds, iteration:  286000\n",
      "Loss is:  0.16473836 ,  25137  seconds, iteration:  287000\n",
      "Loss is:  0.17299946 ,  25233  seconds, iteration:  288000\n",
      "Loss is:  0.14207996 ,  25329  seconds, iteration:  289000\n",
      "Loss is:  0.29819313 ,  25425  seconds, iteration:  290000\n",
      "Loss is:  0.09078858 ,  25521  seconds, iteration:  291000\n",
      "Loss is:  0.03545466 ,  25617  seconds, iteration:  292000\n",
      "Loss is:  0.46477103 ,  25711  seconds, iteration:  293000\n",
      "Loss is:  0.26849577 ,  25806  seconds, iteration:  294000\n",
      "Loss is:  0.1948148 ,  25902  seconds, iteration:  295000\n",
      "Loss is:  0.06398646 ,  25997  seconds, iteration:  296000\n",
      "Loss is:  0.37778068 ,  26093  seconds, iteration:  297000\n",
      "Loss is:  0.15315269 ,  26188  seconds, iteration:  298000\n",
      "Loss is:  0.121391654 ,  26284  seconds, iteration:  299000\n",
      "Loss is:  0.117205776 ,  26380  seconds, iteration:  300000\n",
      "Loss is:  0.318795 ,  26476  seconds, iteration:  301000\n",
      "Loss is:  0.051110268 ,  26572  seconds, iteration:  302000\n",
      "Loss is:  0.18847078 ,  26668  seconds, iteration:  303000\n",
      "Loss is:  0.42997766 ,  26765  seconds, iteration:  304000\n",
      "Loss is:  0.05369136 ,  26861  seconds, iteration:  305000\n",
      "Loss is:  0.17549713 ,  26957  seconds, iteration:  306000\n",
      "Loss is:  0.09501237 ,  27053  seconds, iteration:  307000\n",
      "Loss is:  0.39899874 ,  27150  seconds, iteration:  308000\n",
      "Loss is:  0.0823266 ,  27246  seconds, iteration:  309000\n",
      "Loss is:  0.21257491 ,  27342  seconds, iteration:  310000\n",
      "Loss is:  0.14453506 ,  27438  seconds, iteration:  311000\n",
      "Loss is:  0.11931888 ,  27534  seconds, iteration:  312000\n",
      "Loss is:  0.12558688 ,  27631  seconds, iteration:  313000\n",
      "Loss is:  0.1496989 ,  27727  seconds, iteration:  314000\n",
      "Loss is:  0.09743754 ,  27823  seconds, iteration:  315000\n",
      "Loss is:  0.041056577 ,  27919  seconds, iteration:  316000\n",
      "Loss is:  0.091558255 ,  28015  seconds, iteration:  317000\n",
      "Loss is:  0.13051353 ,  28111  seconds, iteration:  318000\n",
      "Loss is:  0.22420436 ,  28207  seconds, iteration:  319000\n",
      "Loss is:  0.15112206 ,  28303  seconds, iteration:  320000\n",
      "Loss is:  0.057940334 ,  28399  seconds, iteration:  321000\n",
      "Loss is:  0.38743088 ,  28495  seconds, iteration:  322000\n",
      "Loss is:  0.04682851 ,  28591  seconds, iteration:  323000\n",
      "Loss is:  0.3085868 ,  28687  seconds, iteration:  324000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    # Create the graph\n",
    "    sentimentGraph = SentimentGraph()\n",
    "    sentimentGraph.CreateGraph()\n",
    "\n",
    "    # Initialize the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train model\n",
    "    TrainModel(sess, logdir, sentimentGraph)\n",
    "    \n",
    "    # Save the model variables\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, pathToCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prior to validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "resetDevTestIndicies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/matt/w266_saved/binaryclassifier/model.ckpt\n",
      "Testing (Dev) Results:\n",
      "The average accuracy is:  0.9109731\n",
      "The average loss is:  0.22690444\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Create the graph\n",
    "    sentimentGraph = SentimentGraph()\n",
    "    sentimentGraph.CreateGraph()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, pathToCheckpoint)\n",
    "\n",
    "    # Validate model\n",
    "    TestModelDev(sess, logdir, sentimentGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/matt/w266_saved/binaryclassifier/model.ckpt\n",
      "Testing (Test) Results:\n",
      "The average accuracy is:  0.9100816\n",
      "The average loss is:  0.23001097\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Create the graph\n",
    "    sentimentGraph = SentimentGraph()\n",
    "    sentimentGraph.CreateGraph()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, pathToCheckpoint)\n",
    "\n",
    "    # Test model\n",
    "    TestModelTest(sess, logdir, sentimentGraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
