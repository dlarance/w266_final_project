{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Sentiment Classification of Amazon Food Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 25\n",
    "numClasses = 5  # Multi-class classification\n",
    "hiddenSize = 50\n",
    "\n",
    "assert(batchSize % numClasses == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "wordsList = np.load(os.path.join(str(Path.home()), '.kaggle/wordvectors/pretrained_glove/wordsList.npy'))\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(str(Path.home()), '.kaggle/wordvectors/pretrained_glove/wordVectors.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors have dimension 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('~/.kaggle/datasets/snap/amazon-fine-food-reviews/Reviews.csv', encoding='utf8')\n",
    "review_df = review_df.drop(['ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the one, two, three, four, and five star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df = review_df[review_df.Score == 1]\n",
    "one_df.reset_index(inplace=True)\n",
    "\n",
    "two_df = review_df[review_df.Score == 2]\n",
    "two_df.reset_index(inplace=True)\n",
    "\n",
    "three_df = review_df[review_df.Score == 3]\n",
    "three_df.reset_index(inplace=True)\n",
    "\n",
    "four_df = review_df[review_df.Score == 4]\n",
    "four_df.reset_index(inplace=True)\n",
    "\n",
    "five_df = review_df[review_df.Score == 5]\n",
    "five_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the smallest number of reviews for a rating, and that will be the size for all ratings (this ensures class balance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_sizes = [len(one_df), len(two_df), len(three_df), len(four_df), len(five_df)]\n",
    "max_num_ratings = rating_sizes[np.argmin(rating_sizes)]\n",
    "\n",
    "max_num_ratings = 12500  # Limit number of ratings for development\n",
    "\n",
    "one_df   = one_df[0:max_num_ratings]\n",
    "two_df   = two_df[0:max_num_ratings]\n",
    "three_df = three_df[0:max_num_ratings]\n",
    "four_df  = four_df[0:max_num_ratings]\n",
    "five_df  = five_df[0:max_num_ratings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the size of the train, dev, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Train with 60%, , Dev: 10%, Test: 30%\n",
    "train_percent = 0.6\n",
    "dev_percent = 0.1\n",
    "test_percent = 0.3\n",
    "\n",
    "# Get indicies of the rows in the dataframe for training and testing\n",
    "train_lower_index = 0\n",
    "dev_lower_index   = math.floor(train_percent*max_num_ratings)\n",
    "test_lower_index  = math.floor( (train_percent+dev_percent)*max_num_ratings )\n",
    "\n",
    "train_size = dev_lower_index - train_lower_index\n",
    "dev_size   = test_lower_index - dev_lower_index\n",
    "test_size  = max_num_ratings - test_lower_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation, lowercase, and then tokenize the reviews.  The tokens need to be lowercase for the embedding lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return word_tokenize(re.sub(strip_special_chars, \" \", string.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df['Tokens'] = one_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "two_df['Tokens'] = two_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "three_df['Tokens'] = three_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "four_df['Tokens'] = four_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "five_df['Tokens'] = five_df['Text'].apply(lambda text: cleanSentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the array of input sentences converted to word IDs. \n",
    "One extra integer to store the review ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 267  # From EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = np.zeros((5*max_num_ratings, maxSeqLength+1), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert words to word IDs and store in word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = 0\n",
    "\n",
    "for df in [one_df, two_df, three_df, four_df, five_df]:\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        # Store the review Id for identifying misclassified reviews in testing\n",
    "        word_index = 0\n",
    "        word_ids[sentence_index][word_index] = row['Id']\n",
    "        word_index = word_index + 1\n",
    "\n",
    "        for word in row['Tokens']:\n",
    "\n",
    "            try:\n",
    "                word_ids[sentence_index][word_index] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                word_ids[sentence_index][word_index] = 399999 #Vector for unkown words\n",
    "\n",
    "            word_index = word_index + 1\n",
    "\n",
    "            if word_index == maxSeqLength:\n",
    "                break\n",
    "\n",
    "        sentence_index = sentence_index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get the train and test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getBalancedReviews(sectionOffset, sectionSize):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        rating = i % 5\n",
    "        \n",
    "        if (rating == 0): \n",
    "            num = randint(0,sectionSize-1)\n",
    "            labels.append([1, 0, 0, 0, 0])\n",
    "        elif (rating == 1): \n",
    "            num = randint(1*sectionSize,2*sectionSize-1)\n",
    "            labels.append([0, 1, 0, 0, 0])\n",
    "        elif (rating== 2): \n",
    "            num = randint(2*sectionSize,3*sectionSize-1)\n",
    "            labels.append([0, 0, 1, 0, 0])\n",
    "        elif (rating == 3): \n",
    "            num = randint(3*sectionSize,4*sectionSize-1)\n",
    "            labels.append([0, 0, 0, 1, 0])\n",
    "        elif (rating == 4): \n",
    "            num = randint(4*sectionSize,5*sectionSize-1)\n",
    "            labels.append([0, 0, 0, 0, 1])\n",
    "        \n",
    "        num = num + sectionOffset\n",
    "        arr[i] = word_ids[num, 1:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        \n",
    "    return arr, labels, ids\n",
    "\n",
    "def getRandomReviews(sectionOffset, sectionSize):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        num = randint(0, 5*sectionSize-1)\n",
    "        \n",
    "        if (num < sectionSize): \n",
    "            labels.append([1, 0, 0, 0, 0])\n",
    "        elif (num < 2*sectionSize): \n",
    "            labels.append([0, 1, 0, 0, 0])\n",
    "        elif (num < 3*sectionSize): \n",
    "            labels.append([0, 0, 1, 0, 0])\n",
    "        elif (num < 4*sectionSize): \n",
    "            labels.append([0, 0, 0, 1, 0])\n",
    "        elif (num < 5*sectionSize): \n",
    "            labels.append([0, 0, 0, 0, 1])\n",
    "        \n",
    "        num = num + sectionOffset\n",
    "        arr[i] = word_ids[num, 1:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        \n",
    "    return arr, labels, ids\n",
    "\n",
    "def getTrainBatch():\n",
    "    return getBalancedReviews(train_lower_index, train_size)\n",
    "\n",
    "def getDevBatch():\n",
    "    return getRandomReviews(dev_lower_index, dev_size)\n",
    "\n",
    "def getTestBatch():\n",
    "    return getRandomReviews(test_lower_index, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =  0.001\n",
    "dropout_keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-62f659ee654a>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, embedding_dimension]), dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(hiddenSize)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropout_keep_prob)\n",
    "rnn_out, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "W_out = tf.Variable(tf.truncated_normal([hiddenSize, numClasses]), dtype=tf.float32)\n",
    "b_out = tf.Variable(tf.constant(0.1, shape=[numClasses]), dtype=tf.float32)\n",
    "\n",
    "# Get the output of the last RNN cell\n",
    "rnn_out = tf.transpose(rnn_out, [1, 0, 2])\n",
    "last_cell_out = tf.gather(rnn_out, int(rnn_out.get_shape()[0]) - 1)\n",
    "\n",
    "# Calculate logits\n",
    "logits = (tf.matmul(last_cell_out, W_out) + b_out)\n",
    "\n",
    "# Calculate prediction and accuracy\n",
    "prediction = tf.argmax(logits,1)\n",
    "correctPred = tf.equal(prediction, tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def TrainModel(session, writer):\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    i = 0  # Must stay outside the loops\n",
    "    \n",
    "    for epoch in range(460000):\n",
    "        \n",
    "        # Next Batch of reviews\n",
    "        nextBatch, nextBatchLabels, reviewIds = getTrainBatch()\n",
    "\n",
    "        feed_dict_ = {\n",
    "            input_data: nextBatch,\n",
    "            labels: nextBatchLabels\n",
    "        }\n",
    "\n",
    "        loss_, _ = session.run([loss, optimizer], feed_dict=feed_dict_)\n",
    "\n",
    "        #Write summary to Tensorboard\n",
    "        if (i % 10 == 0):\n",
    "            summary = session.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "            writer.add_summary(summary, i)\n",
    "\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"Loss is: \", loss_, \", \", (datetime.datetime.now() - start_time).seconds, \" seconds\")\n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def TestModel(session):\n",
    "    \n",
    "    with open('Mispredicted_AmazonMulticlassClassification.csv', 'w') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        csvwriter.writerow(['Id', 'Prediction', 'Label'])\n",
    "\n",
    "        accuracy_measurements = []\n",
    "        loss_measurements = []\n",
    "\n",
    "        for epoch in range(20):\n",
    "\n",
    "            nextBatch, nextBatchLabels, reviewIds = getTestBatch()\n",
    "\n",
    "            feed_dict = {\n",
    "                input_data: nextBatch,\n",
    "                labels: nextBatchLabels\n",
    "            }\n",
    "\n",
    "            accuracy_, loss_ = sess.run([accuracy, loss], feed_dict)\n",
    "\n",
    "            accuracy_measurements.append(accuracy_)\n",
    "            loss_measurements.append(loss_)\n",
    "\n",
    "            if accuracy_ < 1.0:\n",
    "\n",
    "                predictions_ = sess.run(prediction, feed_dict)\n",
    "\n",
    "                for index in range(len(predictions_)):\n",
    "\n",
    "                    if predictions_[index] != np.argmax(nextBatchLabels[index]):\n",
    "                        csvwriter.writerow([str(int(reviewIds[index])), predictions_[index], np.argmax(nextBatchLabels[index])])\n",
    "\n",
    "        print('Testing Results:')                    \n",
    "        print('The average accuracy is: ', np.mean(accuracy_measurements))\n",
    "        print('The average loss is: ', np.mean(loss_measurements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following:\n",
    "tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  1.5974542 ,  0  seconds\n",
      "Loss is:  1.6129792 ,  64  seconds\n",
      "Loss is:  1.6157428 ,  139  seconds\n",
      "Loss is:  1.6189922 ,  215  seconds\n",
      "Loss is:  1.6019918 ,  290  seconds\n",
      "Loss is:  1.6157156 ,  365  seconds\n",
      "Loss is:  1.6420321 ,  440  seconds\n",
      "Loss is:  1.5744835 ,  516  seconds\n",
      "Loss is:  1.567957 ,  591  seconds\n",
      "Loss is:  1.5659775 ,  667  seconds\n",
      "Loss is:  1.4998808 ,  742  seconds\n",
      "Loss is:  1.6037674 ,  817  seconds\n",
      "Loss is:  1.6265559 ,  892  seconds\n",
      "Loss is:  1.5106865 ,  968  seconds\n",
      "Loss is:  1.5623639 ,  1043  seconds\n",
      "Loss is:  1.4823688 ,  1118  seconds\n",
      "Loss is:  1.6087134 ,  1194  seconds\n",
      "Loss is:  1.5349631 ,  1270  seconds\n",
      "Loss is:  1.6410753 ,  1345  seconds\n",
      "Loss is:  1.6337552 ,  1420  seconds\n",
      "Loss is:  1.6726781 ,  1496  seconds\n",
      "Loss is:  1.4687705 ,  1571  seconds\n",
      "Loss is:  1.5487854 ,  1646  seconds\n",
      "Loss is:  1.6021352 ,  1722  seconds\n",
      "Loss is:  1.4850817 ,  1797  seconds\n",
      "Loss is:  1.4980687 ,  1872  seconds\n",
      "Loss is:  1.5117369 ,  1941  seconds\n",
      "Loss is:  1.6111406 ,  2000  seconds\n",
      "Loss is:  1.5002102 ,  2059  seconds\n",
      "Loss is:  1.5664693 ,  2118  seconds\n",
      "Loss is:  1.5777962 ,  2177  seconds\n",
      "Loss is:  1.5147542 ,  2237  seconds\n",
      "Loss is:  1.6137239 ,  2296  seconds\n",
      "Loss is:  1.5964525 ,  2355  seconds\n",
      "Loss is:  1.5365007 ,  2414  seconds\n",
      "Loss is:  1.5974232 ,  2473  seconds\n",
      "Loss is:  1.570298 ,  2532  seconds\n",
      "Loss is:  1.4930468 ,  2603  seconds\n",
      "Loss is:  1.5278296 ,  2677  seconds\n",
      "Loss is:  1.4929268 ,  2752  seconds\n",
      "Loss is:  1.4830308 ,  2826  seconds\n",
      "Loss is:  1.4847702 ,  2901  seconds\n",
      "Loss is:  1.612567 ,  2975  seconds\n",
      "Loss is:  1.6078839 ,  3050  seconds\n",
      "Loss is:  1.4544687 ,  3124  seconds\n",
      "Loss is:  1.6142792 ,  3199  seconds\n",
      "Loss is:  1.554838 ,  3268  seconds\n",
      "Loss is:  1.5598925 ,  3327  seconds\n",
      "Loss is:  1.6449258 ,  3386  seconds\n",
      "Loss is:  1.394448 ,  3445  seconds\n",
      "Loss is:  1.4860913 ,  3504  seconds\n",
      "Loss is:  1.5042948 ,  3563  seconds\n",
      "Loss is:  1.4142374 ,  3622  seconds\n",
      "Loss is:  1.5437193 ,  3681  seconds\n",
      "Loss is:  1.5601 ,  3740  seconds\n",
      "Loss is:  1.6144534 ,  3799  seconds\n",
      "Loss is:  1.680125 ,  3858  seconds\n",
      "Loss is:  1.6095526 ,  3922  seconds\n",
      "Loss is:  1.4316646 ,  3998  seconds\n",
      "Loss is:  1.446897 ,  4073  seconds\n",
      "Loss is:  1.5133365 ,  4143  seconds\n",
      "Loss is:  1.5812361 ,  4202  seconds\n",
      "Loss is:  1.6725975 ,  4262  seconds\n",
      "Loss is:  1.4242067 ,  4321  seconds\n",
      "Loss is:  1.5198257 ,  4380  seconds\n",
      "Loss is:  1.6007094 ,  4439  seconds\n",
      "Loss is:  1.4834204 ,  4498  seconds\n",
      "Loss is:  1.5534645 ,  4557  seconds\n",
      "Loss is:  1.6486511 ,  4616  seconds\n",
      "Loss is:  1.5294424 ,  4675  seconds\n",
      "Loss is:  1.4683461 ,  4734  seconds\n",
      "Loss is:  1.511873 ,  4802  seconds\n",
      "Loss is:  1.491922 ,  4876  seconds\n",
      "Loss is:  1.445573 ,  4951  seconds\n",
      "Loss is:  1.6801934 ,  5025  seconds\n",
      "Loss is:  1.4877396 ,  5100  seconds\n",
      "Loss is:  1.6194726 ,  5174  seconds\n",
      "Loss is:  1.3865371 ,  5249  seconds\n",
      "Loss is:  1.5061723 ,  5323  seconds\n",
      "Loss is:  1.357173 ,  5398  seconds\n",
      "Loss is:  1.5792025 ,  5472  seconds\n",
      "Loss is:  1.5435454 ,  5546  seconds\n",
      "Loss is:  1.4473494 ,  5621  seconds\n",
      "Loss is:  1.4122101 ,  5695  seconds\n",
      "Loss is:  1.5978084 ,  5769  seconds\n",
      "Loss is:  1.6193795 ,  5844  seconds\n",
      "Loss is:  1.741772 ,  5919  seconds\n",
      "Loss is:  1.5632521 ,  5993  seconds\n",
      "Loss is:  1.4622436 ,  6068  seconds\n",
      "Loss is:  1.4960464 ,  6143  seconds\n",
      "Loss is:  1.426998 ,  6218  seconds\n",
      "Loss is:  1.5006908 ,  6294  seconds\n",
      "Loss is:  1.5514973 ,  6369  seconds\n",
      "Loss is:  1.4708098 ,  6445  seconds\n",
      "Loss is:  1.4303073 ,  6521  seconds\n",
      "Loss is:  1.5384078 ,  6596  seconds\n",
      "Loss is:  1.4163256 ,  6670  seconds\n",
      "Loss is:  1.4680332 ,  6745  seconds\n",
      "Loss is:  1.5952629 ,  6820  seconds\n",
      "Loss is:  1.5658956 ,  6895  seconds\n",
      "Loss is:  1.480844 ,  6969  seconds\n",
      "Loss is:  1.4813694 ,  7044  seconds\n",
      "Loss is:  1.5583942 ,  7119  seconds\n",
      "Loss is:  1.5219684 ,  7194  seconds\n",
      "Loss is:  1.4517866 ,  7268  seconds\n",
      "Loss is:  1.6189655 ,  7343  seconds\n",
      "Loss is:  1.4998401 ,  7418  seconds\n",
      "Loss is:  1.5748631 ,  7493  seconds\n",
      "Loss is:  1.5593225 ,  7567  seconds\n",
      "Loss is:  1.6493694 ,  7642  seconds\n",
      "Loss is:  1.6815282 ,  7717  seconds\n",
      "Loss is:  1.4082961 ,  7793  seconds\n",
      "Loss is:  1.3177437 ,  7868  seconds\n",
      "Loss is:  1.4019895 ,  7944  seconds\n",
      "Loss is:  1.5045133 ,  8019  seconds\n",
      "Loss is:  1.5096271 ,  8095  seconds\n",
      "Loss is:  1.3969967 ,  8170  seconds\n",
      "Loss is:  1.4035465 ,  8246  seconds\n",
      "Loss is:  1.5041386 ,  8322  seconds\n",
      "Loss is:  1.3165137 ,  8397  seconds\n",
      "Loss is:  1.457209 ,  8472  seconds\n",
      "Loss is:  1.5482658 ,  8548  seconds\n",
      "Loss is:  1.4540097 ,  8623  seconds\n",
      "Loss is:  1.5071118 ,  8699  seconds\n",
      "Loss is:  1.5276941 ,  8774  seconds\n",
      "Loss is:  1.603325 ,  8849  seconds\n",
      "Loss is:  1.4881217 ,  8925  seconds\n",
      "Loss is:  1.4035553 ,  9000  seconds\n",
      "Loss is:  1.4372864 ,  9076  seconds\n",
      "Loss is:  1.3958392 ,  9151  seconds\n",
      "Loss is:  1.3905681 ,  9226  seconds\n",
      "Loss is:  1.406869 ,  9301  seconds\n",
      "Loss is:  1.4862398 ,  9376  seconds\n",
      "Loss is:  1.4644731 ,  9452  seconds\n",
      "Loss is:  1.4903351 ,  9527  seconds\n",
      "Loss is:  1.2864769 ,  9602  seconds\n",
      "Loss is:  1.4915681 ,  9677  seconds\n",
      "Loss is:  1.5312643 ,  9752  seconds\n",
      "Loss is:  1.7275455 ,  9828  seconds\n",
      "Loss is:  1.4014657 ,  9903  seconds\n",
      "Loss is:  1.444937 ,  9966  seconds\n",
      "Loss is:  1.424734 ,  10025  seconds\n",
      "Loss is:  1.3975213 ,  10084  seconds\n",
      "Loss is:  1.4046359 ,  10144  seconds\n",
      "Loss is:  1.5980439 ,  10203  seconds\n",
      "Loss is:  1.290706 ,  10262  seconds\n",
      "Loss is:  1.5916616 ,  10321  seconds\n",
      "Loss is:  1.3287903 ,  10380  seconds\n",
      "Loss is:  1.6021091 ,  10439  seconds\n",
      "Loss is:  1.4357045 ,  10498  seconds\n",
      "Loss is:  1.3244189 ,  10564  seconds\n",
      "Loss is:  1.4429814 ,  10639  seconds\n",
      "Loss is:  1.5131127 ,  10713  seconds\n",
      "Loss is:  1.445541 ,  10788  seconds\n",
      "Loss is:  1.4101442 ,  10863  seconds\n",
      "Loss is:  1.1334323 ,  10938  seconds\n",
      "Loss is:  1.4308516 ,  11013  seconds\n",
      "Loss is:  1.3431809 ,  11088  seconds\n",
      "Loss is:  1.4117994 ,  11163  seconds\n",
      "Loss is:  1.3781734 ,  11237  seconds\n",
      "Loss is:  1.4110762 ,  11312  seconds\n",
      "Loss is:  1.1907425 ,  11388  seconds\n",
      "Loss is:  1.4864616 ,  11463  seconds\n",
      "Loss is:  1.3785605 ,  11538  seconds\n",
      "Loss is:  1.4282645 ,  11613  seconds\n",
      "Loss is:  1.4970367 ,  11688  seconds\n",
      "Loss is:  1.3981444 ,  11763  seconds\n",
      "Loss is:  1.4158583 ,  11838  seconds\n",
      "Loss is:  1.3350685 ,  11913  seconds\n",
      "Loss is:  1.490435 ,  11987  seconds\n",
      "Loss is:  1.4969969 ,  12063  seconds\n",
      "Loss is:  1.4580036 ,  12137  seconds\n",
      "Loss is:  1.4183965 ,  12212  seconds\n",
      "Loss is:  1.4364572 ,  12287  seconds\n",
      "Loss is:  1.5052304 ,  12362  seconds\n",
      "Loss is:  1.2722104 ,  12437  seconds\n",
      "Loss is:  1.4498106 ,  12512  seconds\n",
      "Loss is:  1.2744533 ,  12587  seconds\n",
      "Loss is:  1.2735796 ,  12663  seconds\n",
      "Loss is:  1.0947258 ,  12739  seconds\n",
      "Loss is:  1.3961465 ,  12814  seconds\n",
      "Loss is:  1.5382496 ,  12889  seconds\n",
      "Loss is:  1.3084995 ,  12965  seconds\n",
      "Loss is:  1.3723037 ,  13040  seconds\n",
      "Loss is:  1.2854095 ,  13115  seconds\n",
      "Loss is:  1.2609549 ,  13191  seconds\n",
      "Loss is:  1.2203295 ,  13265  seconds\n",
      "Loss is:  1.3328083 ,  13340  seconds\n",
      "Loss is:  1.5670902 ,  13415  seconds\n",
      "Loss is:  1.2629192 ,  13474  seconds\n",
      "Loss is:  1.5611672 ,  13533  seconds\n",
      "Loss is:  1.4782901 ,  13592  seconds\n",
      "Loss is:  1.5492047 ,  13651  seconds\n",
      "Loss is:  1.4199638 ,  13710  seconds\n",
      "Loss is:  1.3726577 ,  13769  seconds\n",
      "Loss is:  1.3285468 ,  13828  seconds\n",
      "Loss is:  1.3392106 ,  13888  seconds\n",
      "Loss is:  1.2118132 ,  13947  seconds\n",
      "Loss is:  1.273437 ,  14006  seconds\n",
      "Loss is:  1.2063516 ,  14074  seconds\n",
      "Loss is:  1.2698501 ,  14149  seconds\n",
      "Loss is:  1.2010312 ,  14225  seconds\n",
      "Loss is:  1.2287389 ,  14300  seconds\n",
      "Loss is:  1.4891356 ,  14375  seconds\n",
      "Loss is:  1.0792819 ,  14451  seconds\n",
      "Loss is:  1.2967294 ,  14526  seconds\n",
      "Loss is:  1.2389857 ,  14602  seconds\n",
      "Loss is:  1.4321649 ,  14677  seconds\n",
      "Loss is:  1.1393831 ,  14752  seconds\n",
      "Loss is:  1.6303856 ,  14827  seconds\n",
      "Loss is:  1.1752478 ,  14903  seconds\n",
      "Loss is:  1.2694706 ,  14978  seconds\n",
      "Loss is:  1.6068558 ,  15054  seconds\n",
      "Loss is:  1.2897223 ,  15129  seconds\n",
      "Loss is:  1.3893448 ,  15205  seconds\n",
      "Loss is:  1.4238046 ,  15280  seconds\n",
      "Loss is:  1.4345165 ,  15355  seconds\n",
      "Loss is:  1.3498796 ,  15431  seconds\n",
      "Loss is:  1.4476922 ,  15507  seconds\n",
      "Loss is:  1.4900264 ,  15582  seconds\n",
      "Loss is:  1.3169552 ,  15658  seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  1.5558214 ,  15733  seconds\n",
      "Loss is:  1.5891736 ,  15809  seconds\n",
      "Loss is:  1.4604187 ,  15884  seconds\n",
      "Loss is:  1.2325226 ,  15959  seconds\n",
      "Loss is:  1.4575728 ,  16035  seconds\n",
      "Loss is:  1.3189762 ,  16110  seconds\n",
      "Loss is:  1.3500596 ,  16185  seconds\n",
      "Loss is:  1.3199565 ,  16260  seconds\n",
      "Loss is:  1.4903914 ,  16335  seconds\n",
      "Loss is:  1.3823509 ,  16410  seconds\n",
      "Loss is:  1.3829068 ,  16485  seconds\n",
      "Loss is:  1.1735368 ,  16561  seconds\n",
      "Loss is:  1.374049 ,  16636  seconds\n",
      "Loss is:  1.655516 ,  16711  seconds\n",
      "Loss is:  1.3576882 ,  16786  seconds\n",
      "Loss is:  1.4662632 ,  16862  seconds\n",
      "Loss is:  1.4408317 ,  16937  seconds\n",
      "Loss is:  1.3357774 ,  17012  seconds\n",
      "Loss is:  1.3333054 ,  17087  seconds\n",
      "Loss is:  1.332624 ,  17162  seconds\n",
      "Loss is:  1.2897158 ,  17236  seconds\n",
      "Loss is:  1.0886744 ,  17311  seconds\n",
      "Loss is:  1.4027482 ,  17386  seconds\n",
      "Loss is:  0.95398486 ,  17461  seconds\n",
      "Loss is:  0.9918543 ,  17535  seconds\n",
      "Loss is:  1.1583493 ,  17610  seconds\n",
      "Loss is:  1.1805294 ,  17685  seconds\n",
      "Loss is:  1.3212225 ,  17761  seconds\n",
      "Loss is:  1.2910643 ,  17836  seconds\n",
      "Loss is:  1.3373386 ,  17911  seconds\n",
      "Loss is:  1.3272488 ,  17986  seconds\n",
      "Loss is:  1.3417033 ,  18062  seconds\n",
      "Loss is:  1.2409482 ,  18137  seconds\n",
      "Loss is:  1.1637424 ,  18212  seconds\n",
      "Loss is:  1.2563325 ,  18287  seconds\n",
      "Loss is:  1.4183066 ,  18362  seconds\n",
      "Loss is:  1.1621917 ,  18436  seconds\n",
      "Loss is:  1.2495384 ,  18511  seconds\n",
      "Loss is:  1.1598442 ,  18586  seconds\n",
      "Loss is:  1.1338485 ,  18661  seconds\n",
      "Loss is:  1.2393876 ,  18736  seconds\n",
      "Loss is:  1.1825624 ,  18811  seconds\n",
      "Loss is:  1.3105942 ,  18886  seconds\n",
      "Loss is:  1.1229768 ,  18961  seconds\n",
      "Loss is:  1.1612868 ,  19037  seconds\n",
      "Loss is:  1.1413178 ,  19112  seconds\n",
      "Loss is:  1.2981268 ,  19187  seconds\n",
      "Loss is:  1.6488216 ,  19262  seconds\n",
      "Loss is:  1.3822598 ,  19337  seconds\n",
      "Loss is:  1.0175407 ,  19412  seconds\n",
      "Loss is:  1.1605082 ,  19487  seconds\n",
      "Loss is:  1.411535 ,  19562  seconds\n",
      "Loss is:  1.3333637 ,  19638  seconds\n",
      "Loss is:  1.3470861 ,  19713  seconds\n",
      "Loss is:  1.0727551 ,  19788  seconds\n",
      "Loss is:  1.1764235 ,  19862  seconds\n",
      "Loss is:  1.7282577 ,  19937  seconds\n",
      "Loss is:  1.4673828 ,  20012  seconds\n",
      "Loss is:  1.338199 ,  20087  seconds\n",
      "Loss is:  1.3127697 ,  20162  seconds\n",
      "Loss is:  1.108895 ,  20236  seconds\n",
      "Loss is:  1.1725811 ,  20311  seconds\n",
      "Loss is:  1.2398125 ,  20386  seconds\n",
      "Loss is:  1.3050972 ,  20461  seconds\n",
      "Loss is:  1.1214936 ,  20536  seconds\n",
      "Loss is:  1.2709652 ,  20611  seconds\n",
      "Loss is:  1.2282723 ,  20685  seconds\n",
      "Loss is:  1.2641463 ,  20760  seconds\n",
      "Loss is:  1.4219207 ,  20834  seconds\n",
      "Loss is:  1.0628555 ,  20908  seconds\n",
      "Loss is:  1.0664366 ,  20983  seconds\n",
      "Loss is:  1.1071389 ,  21058  seconds\n",
      "Loss is:  1.4479423 ,  21132  seconds\n",
      "Loss is:  1.2582848 ,  21207  seconds\n",
      "Loss is:  1.1610382 ,  21281  seconds\n",
      "Loss is:  1.3214508 ,  21356  seconds\n",
      "Loss is:  1.2824674 ,  21431  seconds\n",
      "Loss is:  1.2054173 ,  21506  seconds\n",
      "Loss is:  1.1277213 ,  21580  seconds\n",
      "Loss is:  1.1954659 ,  21655  seconds\n",
      "Loss is:  1.099885 ,  21730  seconds\n",
      "Loss is:  1.3586807 ,  21804  seconds\n",
      "Loss is:  1.0321203 ,  21879  seconds\n",
      "Loss is:  1.046449 ,  21954  seconds\n",
      "Loss is:  1.0580308 ,  22029  seconds\n",
      "Loss is:  1.0716614 ,  22103  seconds\n",
      "Loss is:  1.2474394 ,  22179  seconds\n",
      "Loss is:  1.4875079 ,  22254  seconds\n",
      "Loss is:  1.3958539 ,  22329  seconds\n",
      "Loss is:  1.2075794 ,  22405  seconds\n",
      "Loss is:  1.2551733 ,  22480  seconds\n",
      "Loss is:  1.3065016 ,  22555  seconds\n",
      "Loss is:  1.2016184 ,  22631  seconds\n",
      "Loss is:  1.2735531 ,  22706  seconds\n",
      "Loss is:  1.3528104 ,  22781  seconds\n",
      "Loss is:  1.4422159 ,  22856  seconds\n",
      "Loss is:  1.24016 ,  22931  seconds\n",
      "Loss is:  1.2344995 ,  23006  seconds\n",
      "Loss is:  1.1201224 ,  23081  seconds\n",
      "Loss is:  0.9584903 ,  23156  seconds\n",
      "Loss is:  1.1163852 ,  23231  seconds\n",
      "Loss is:  1.2538712 ,  23307  seconds\n",
      "Loss is:  1.1666116 ,  23382  seconds\n",
      "Loss is:  1.5422697 ,  23457  seconds\n",
      "Loss is:  1.7763314 ,  23532  seconds\n",
      "Loss is:  1.3482213 ,  23606  seconds\n",
      "Loss is:  1.2779313 ,  23682  seconds\n",
      "Loss is:  1.2474538 ,  23757  seconds\n",
      "Loss is:  1.392279 ,  23832  seconds\n",
      "Loss is:  1.2358588 ,  23907  seconds\n",
      "Loss is:  1.361599 ,  23982  seconds\n",
      "Loss is:  1.4956999 ,  24057  seconds\n",
      "Loss is:  1.4226773 ,  24132  seconds\n",
      "Loss is:  0.93062747 ,  24206  seconds\n",
      "Loss is:  1.2976632 ,  24282  seconds\n",
      "Loss is:  0.99056125 ,  24356  seconds\n",
      "Loss is:  1.083838 ,  24431  seconds\n",
      "Loss is:  1.0209074 ,  24505  seconds\n",
      "Loss is:  1.1189774 ,  24581  seconds\n",
      "Loss is:  1.0722821 ,  24656  seconds\n",
      "Loss is:  1.0933992 ,  24731  seconds\n",
      "Loss is:  1.1708463 ,  24806  seconds\n",
      "Loss is:  1.1056261 ,  24881  seconds\n",
      "Loss is:  1.2723979 ,  24956  seconds\n",
      "Loss is:  1.2806166 ,  25030  seconds\n",
      "Loss is:  0.9393205 ,  25105  seconds\n",
      "Loss is:  1.0275588 ,  25180  seconds\n",
      "Loss is:  1.0936937 ,  25254  seconds\n",
      "Loss is:  1.0938917 ,  25329  seconds\n",
      "Loss is:  1.6773095 ,  25403  seconds\n",
      "Loss is:  1.414582 ,  25479  seconds\n",
      "Loss is:  1.0217643 ,  25554  seconds\n",
      "Loss is:  0.9777789 ,  25628  seconds\n",
      "Loss is:  1.1827002 ,  25703  seconds\n",
      "Loss is:  1.239364 ,  25779  seconds\n",
      "Loss is:  1.2347294 ,  25853  seconds\n",
      "Loss is:  1.5133339 ,  25928  seconds\n",
      "Loss is:  1.3389115 ,  26003  seconds\n",
      "Loss is:  1.3863298 ,  26078  seconds\n",
      "Loss is:  1.0319939 ,  26152  seconds\n",
      "Loss is:  1.2518724 ,  26227  seconds\n",
      "Loss is:  1.1021457 ,  26301  seconds\n",
      "Loss is:  1.1713625 ,  26376  seconds\n",
      "Loss is:  0.87052596 ,  26451  seconds\n",
      "Loss is:  1.3871707 ,  26525  seconds\n",
      "Loss is:  1.1791892 ,  26600  seconds\n",
      "Loss is:  1.0744426 ,  26675  seconds\n",
      "Loss is:  1.1245687 ,  26749  seconds\n",
      "Loss is:  1.1399907 ,  26824  seconds\n",
      "Loss is:  1.3188574 ,  26898  seconds\n",
      "Loss is:  1.1592913 ,  26972  seconds\n",
      "Loss is:  1.5367602 ,  27047  seconds\n",
      "Loss is:  1.1130809 ,  27122  seconds\n",
      "Loss is:  1.1490984 ,  27196  seconds\n",
      "Loss is:  1.2460341 ,  27271  seconds\n",
      "Loss is:  1.3493881 ,  27345  seconds\n",
      "Loss is:  1.3266654 ,  27420  seconds\n",
      "Loss is:  1.1383071 ,  27495  seconds\n",
      "Loss is:  1.1967502 ,  27569  seconds\n",
      "Loss is:  1.1524104 ,  27644  seconds\n",
      "Loss is:  1.2628692 ,  27719  seconds\n",
      "Loss is:  1.2371694 ,  27794  seconds\n",
      "Loss is:  1.3851303 ,  27869  seconds\n",
      "Loss is:  1.1017832 ,  27944  seconds\n",
      "Loss is:  1.3602389 ,  28019  seconds\n",
      "Loss is:  1.1972176 ,  28094  seconds\n",
      "Loss is:  1.4216969 ,  28169  seconds\n",
      "Loss is:  0.83655304 ,  28245  seconds\n",
      "Loss is:  1.1062641 ,  28320  seconds\n",
      "Loss is:  0.9888929 ,  28395  seconds\n",
      "Loss is:  1.1730309 ,  28470  seconds\n",
      "Loss is:  1.3081397 ,  28545  seconds\n",
      "Loss is:  1.0988672 ,  28620  seconds\n",
      "Loss is:  1.1615199 ,  28695  seconds\n",
      "Loss is:  1.1005161 ,  28770  seconds\n",
      "Loss is:  0.9830543 ,  28845  seconds\n",
      "Loss is:  1.2068782 ,  28920  seconds\n",
      "Loss is:  1.1197224 ,  28995  seconds\n",
      "Loss is:  1.1145729 ,  29070  seconds\n",
      "Loss is:  0.9904421 ,  29145  seconds\n",
      "Loss is:  1.2886653 ,  29220  seconds\n",
      "Loss is:  1.2654594 ,  29295  seconds\n",
      "Loss is:  1.1943668 ,  29369  seconds\n",
      "Loss is:  0.9987697 ,  29445  seconds\n",
      "Loss is:  0.88103956 ,  29520  seconds\n",
      "Loss is:  1.4116576 ,  29596  seconds\n",
      "Loss is:  1.1259998 ,  29671  seconds\n",
      "Loss is:  1.2301619 ,  29746  seconds\n",
      "Loss is:  1.0537406 ,  29822  seconds\n",
      "Loss is:  0.89194834 ,  29897  seconds\n",
      "Loss is:  1.1648568 ,  29972  seconds\n",
      "Loss is:  0.8105957 ,  30048  seconds\n",
      "Loss is:  1.2454158 ,  30123  seconds\n",
      "Loss is:  1.354989 ,  30199  seconds\n",
      "Loss is:  1.3775998 ,  30274  seconds\n",
      "Loss is:  1.340846 ,  30349  seconds\n",
      "Loss is:  1.3938603 ,  30424  seconds\n",
      "Loss is:  1.3382742 ,  30499  seconds\n",
      "Loss is:  0.9046168 ,  30574  seconds\n",
      "Loss is:  0.9808681 ,  30649  seconds\n",
      "Loss is:  1.1236963 ,  30725  seconds\n",
      "Loss is:  1.4889129 ,  30800  seconds\n",
      "Loss is:  1.3000146 ,  30875  seconds\n",
      "Loss is:  1.3580687 ,  30950  seconds\n",
      "Loss is:  1.1013244 ,  31026  seconds\n",
      "Loss is:  1.4437228 ,  31101  seconds\n",
      "Loss is:  1.2517685 ,  31177  seconds\n",
      "Loss is:  1.2217543 ,  31252  seconds\n",
      "Loss is:  1.1716583 ,  31327  seconds\n",
      "Loss is:  0.8401489 ,  31403  seconds\n",
      "Loss is:  1.3965192 ,  31478  seconds\n",
      "Loss is:  1.152905 ,  31553  seconds\n",
      "Loss is:  1.3574562 ,  31628  seconds\n",
      "Loss is:  1.0285093 ,  31703  seconds\n",
      "Loss is:  1.0697209 ,  31778  seconds\n",
      "Loss is:  1.0269258 ,  31853  seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  1.1225617 ,  31929  seconds\n",
      "Loss is:  0.9849096 ,  32004  seconds\n",
      "Loss is:  1.1874005 ,  32079  seconds\n",
      "Loss is:  1.3997965 ,  32154  seconds\n",
      "Loss is:  1.2072213 ,  32229  seconds\n",
      "Loss is:  1.2030987 ,  32304  seconds\n",
      "Loss is:  1.2032427 ,  32380  seconds\n",
      "Loss is:  1.0001949 ,  32455  seconds\n",
      "Loss is:  1.2087003 ,  32530  seconds\n",
      "Loss is:  1.1619962 ,  32606  seconds\n",
      "Loss is:  1.5677428 ,  32681  seconds\n",
      "Loss is:  1.0914494 ,  32756  seconds\n",
      "Loss is:  1.2404009 ,  32831  seconds\n",
      "Loss is:  1.3633217 ,  32906  seconds\n",
      "Loss is:  1.1635797 ,  32982  seconds\n",
      "Loss is:  1.2353723 ,  33057  seconds\n",
      "Loss is:  0.96639985 ,  33133  seconds\n",
      "Loss is:  1.2801841 ,  33208  seconds\n",
      "Loss is:  1.614278 ,  33283  seconds\n",
      "Loss is:  0.9996787 ,  33358  seconds\n",
      "Loss is:  1.2328168 ,  33433  seconds\n",
      "Loss is:  1.2522043 ,  33508  seconds\n",
      "Loss is:  0.88733596 ,  33583  seconds\n",
      "Testing Results:\n",
      "The average accuracy is:  0.30600002\n",
      "The average loss is:  1.7948484\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    tf.summary.scalar('Loss', loss)\n",
    "    tf.summary.scalar('Accuracy', accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "        \n",
    "    # Train model\n",
    "    TrainModel(sess, writer)\n",
    "    \n",
    "    # Test model\n",
    "    TestModel(sess)\n",
    "    \n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
