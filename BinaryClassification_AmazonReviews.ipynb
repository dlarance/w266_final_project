{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Sentiment Classification of Amazon Food Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to serialization files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToBinClassDir = '/tmp/binaryclassifier'\n",
    "pathToWordId      = '/tmp/binaryclassifier/wordId.npy'\n",
    "pathToCheckpoint  = '/tmp/binaryclassifier/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "numClasses = 2  # Binary classification\n",
    "hiddenSize = 50\n",
    "\n",
    "assert(batchSize % numClasses == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "wordsList = np.load(os.path.join(str(Path.home()), '.kaggle/wordvectors/pretrained_glove/wordsList.npy'))\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(str(Path.home()), '.kaggle/wordvectors/pretrained_glove/wordVectors.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors have dimension 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/matt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('~/.kaggle/datasets/snap/amazon-fine-food-reviews/Reviews.csv', encoding='utf8')\n",
    "review_df = review_df.drop(['ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the one and five star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df = review_df[review_df.Score == 1]\n",
    "one_df.reset_index(inplace=True)\n",
    "\n",
    "two_df = review_df[review_df.Score == 2]\n",
    "two_df.reset_index(inplace=True)\n",
    "\n",
    "four_df = review_df[review_df.Score == 4]\n",
    "four_df.reset_index(inplace=True)\n",
    "\n",
    "five_df = review_df[review_df.Score == 5]\n",
    "five_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit number of ratings for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_ratings = 12500\n",
    "one_df = one_df[0:max_num_ratings]\n",
    "two_df = two_df[0:max_num_ratings]\n",
    "four_df = four_df[0:max_num_ratings]\n",
    "five_df = five_df[0:max_num_ratings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the size of the train, dev, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Train with 60%, , Dev: 10%, Test: 30%\n",
    "train_percent = 0.6\n",
    "dev_percent = 0.1\n",
    "test_percent = 0.3\n",
    "\n",
    "# Get indicies of the rows in the dataframe for training and testing\n",
    "train_lower_index = 0\n",
    "dev_lower_index   = math.floor(train_percent*max_num_ratings)\n",
    "test_lower_index  = math.floor( (train_percent+dev_percent)*max_num_ratings )\n",
    "\n",
    "train_size = dev_lower_index - train_lower_index\n",
    "dev_size   = test_lower_index - dev_lower_index\n",
    "test_size  = max_num_ratings - test_lower_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation, lowercase, and then tokenize the reviews.  The tokens need to be lowercase for the embedding lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return word_tokenize(re.sub(strip_special_chars, \" \", string.lower()))\n",
    "\n",
    "one_df['Tokens'] = one_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "two_df['Tokens'] = two_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "four_df['Tokens'] = four_df['Text'].apply(lambda text: cleanSentences(text))\n",
    "five_df['Tokens'] = five_df['Text'].apply(lambda text: cleanSentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the array of input sentences converted to word IDs. \n",
    "One extra integer to store the review ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 267 # From EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = np.zeros((4*max_num_ratings, maxSeqLength+1), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert words to word IDs and store in word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "word_id_file = Path(pathToWordId)\n",
    "\n",
    "if not word_id_file.exists():\n",
    "\n",
    "    sentence_index = 0\n",
    "\n",
    "    for df in [one_df, two_df, four_df, five_df]:\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "\n",
    "            # Store the review Id for identifying misclassified reviews in testing\n",
    "            word_index = 0\n",
    "            word_ids[sentence_index][word_index] = row['Id']\n",
    "            word_index = word_index + 1\n",
    "\n",
    "            for word in row['Tokens']:\n",
    "\n",
    "                try:\n",
    "                    word_ids[sentence_index][word_index] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                    word_ids[sentence_index][word_index] = 399999 #Vector for unkown words\n",
    "\n",
    "                word_index = word_index + 1\n",
    "\n",
    "                if word_index == maxSeqLength:\n",
    "                    break\n",
    "\n",
    "            sentence_index = sentence_index + 1\n",
    "\n",
    "    binClassDir = Path(pathToBinClassDir)\n",
    "    \n",
    "    if not binClassDir.exists():\n",
    "        os.mkdir(pathToBinClassDir)\n",
    "    \n",
    "    np.save(pathToWordId, word_ids)\n",
    "else:\n",
    "    word_ids = np.load(pathToWordId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get the train and test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getBalancedReviews(sectionOffset, sectionSize):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        rating = i % 4\n",
    "        \n",
    "        if (rating == 0): \n",
    "            num = randint(0,sectionSize-1)\n",
    "            labels.append([1, 0])\n",
    "        elif (rating == 1): \n",
    "            num = randint(1*sectionSize,2*sectionSize-1)\n",
    "            labels.append([1, 0])\n",
    "        elif (rating == 2): \n",
    "            num = randint(2*sectionSize,3*sectionSize-1)\n",
    "            labels.append([0, 1])\n",
    "        elif (rating == 3): \n",
    "            num = randint(3*sectionSize,4*sectionSize-1)\n",
    "            labels.append([0, 1])\n",
    "        \n",
    "        num = num + sectionOffset\n",
    "        arr[i] = word_ids[num, 1:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        \n",
    "    return arr, labels, ids\n",
    "\n",
    "def getRandomReviews(sectionOffset, sectionSize):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    ids = np.zeros(batchSize)\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        num = randint(0, 4*sectionSize-1)\n",
    "        \n",
    "        if (num < 2*sectionSize): \n",
    "            labels.append([1, 0])\n",
    "        else: \n",
    "            labels.append([0, 1])\n",
    "        \n",
    "        num = num + sectionOffset\n",
    "        arr[i] = word_ids[num, 1:]\n",
    "        ids[i] = word_ids[num, 0]\n",
    "        \n",
    "    return arr, labels, ids\n",
    "\n",
    "def getTrainBatch():\n",
    "    return getBalancedReviews(train_lower_index, train_size)\n",
    "\n",
    "def getDevBatch():\n",
    "    return getRandomReviews(dev_lower_index, dev_size)\n",
    "\n",
    "def getTestBatch():\n",
    "    return getRandomReviews(test_lower_index, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =  0.001\n",
    "dropout_keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "        self.input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "        self.prediction = None\n",
    "        self.accuracy = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    def CreateGraph(self):\n",
    "        data = tf.Variable(tf.zeros([batchSize, maxSeqLength, embedding_dimension]), dtype=tf.float32)\n",
    "        data = tf.nn.embedding_lookup(wordVectors, self.input_data)\n",
    "\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(hiddenSize)\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropout_keep_prob)\n",
    "        rnn_out, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "        W_out = tf.Variable(tf.truncated_normal([hiddenSize, numClasses]), dtype=tf.float32)\n",
    "        b_out = tf.Variable(tf.constant(0.1, shape=[numClasses]), dtype=tf.float32)\n",
    "\n",
    "        # Get the output of the last RNN cell\n",
    "        rnn_out = tf.transpose(rnn_out, [1, 0, 2])\n",
    "        last_cell_out = tf.gather(rnn_out, int(rnn_out.get_shape()[0]) - 1)\n",
    "\n",
    "        # Calculate logits\n",
    "        logits = (tf.matmul(last_cell_out, W_out) + b_out)\n",
    "\n",
    "        # Calculate prediction and accuracy\n",
    "        self.prediction = tf.argmax(logits,1)\n",
    "        correctPred = tf.equal(self.prediction, tf.argmax(self.labels,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.labels))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def TrainModel(session, graph):\n",
    "    \n",
    "    logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    \n",
    "    # Open the writer\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    \n",
    "    tf.summary.scalar('Loss', graph.loss)\n",
    "    tf.summary.scalar('Accuracy', graph.accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "        \n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    i = 0  # Must stay outside the loops\n",
    "    \n",
    "    for epoch in range(260000):\n",
    "        \n",
    "        # Next Batch of reviews\n",
    "        nextBatch, nextBatchLabels, reviewIds = getTrainBatch()\n",
    "\n",
    "        feed_dict_ = {\n",
    "            graph.input_data: nextBatch,\n",
    "            graph.labels: nextBatchLabels\n",
    "        }\n",
    "\n",
    "        loss_, _ = session.run([graph.loss, graph.optimizer], feed_dict=feed_dict_)\n",
    "\n",
    "        #Write summary to Tensorboard\n",
    "        if (i % 10 == 0):\n",
    "            summary = session.run(merged, {graph.input_data: nextBatch, graph.labels: nextBatchLabels})\n",
    "            writer.add_summary(summary, i)\n",
    "\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"Loss is: \", loss_, \", \", (datetime.datetime.now() - start_time).seconds, \" seconds\")\n",
    "\n",
    "        i = i + 1\n",
    "    \n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModel(session, graph):\n",
    "    \n",
    "    csv = open('Mispredicted_AmazonBinaryClassification.csv', 'w')\n",
    "    csv.write(\"Id\\n\")\n",
    "    \n",
    "    accuracy_measurements = []\n",
    "    loss_measurements = []\n",
    "    \n",
    "    for epoch in range(20):\n",
    "    \n",
    "        nextBatch, nextBatchLabels, reviewIds = getTestBatch()\n",
    "\n",
    "        feed_dict = {\n",
    "            graph.input_data: nextBatch,\n",
    "            graph.labels: nextBatchLabels\n",
    "        }\n",
    "\n",
    "        accuracy_, loss_ = sess.run([graph.accuracy, graph.loss], feed_dict)\n",
    "        \n",
    "        accuracy_measurements.append(accuracy_)\n",
    "        loss_measurements.append(loss_)\n",
    "\n",
    "        if accuracy_ < 1.0:\n",
    "\n",
    "            predictions_ = sess.run(graph.prediction, feed_dict)\n",
    "            \n",
    "            for index in range(len(predictions_)):\n",
    "\n",
    "                if predictions_[index] != np.argmax(nextBatchLabels[index]):\n",
    "                    csv.write(str(int(reviewIds[index])) + \"\\n\")\n",
    "                    \n",
    "    print('Testing Results:')\n",
    "    print('The average accuracy is: ', np.mean(accuracy_measurements))\n",
    "    print('The average loss is: ', np.mean(loss_measurements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following:\n",
    "tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-b69099e871b5>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Loss is:  0.73513705 ,  0  seconds\n",
      "Loss is:  0.6984404 ,  67  seconds\n",
      "Loss is:  0.70522165 ,  133  seconds\n",
      "Loss is:  0.68207407 ,  199  seconds\n",
      "Loss is:  0.7056764 ,  265  seconds\n",
      "Loss is:  0.73364943 ,  331  seconds\n",
      "Loss is:  0.6982748 ,  397  seconds\n",
      "Loss is:  0.6906988 ,  463  seconds\n",
      "Loss is:  0.6323196 ,  530  seconds\n",
      "Loss is:  0.64474493 ,  596  seconds\n",
      "Loss is:  0.6914451 ,  662  seconds\n",
      "Loss is:  0.6795556 ,  728  seconds\n",
      "Loss is:  0.69616675 ,  794  seconds\n",
      "Loss is:  0.7011967 ,  860  seconds\n",
      "Loss is:  0.6767094 ,  926  seconds\n",
      "Loss is:  0.6773462 ,  992  seconds\n",
      "Loss is:  0.67855865 ,  1058  seconds\n",
      "Loss is:  0.6835862 ,  1124  seconds\n",
      "Loss is:  0.65718466 ,  1191  seconds\n",
      "Loss is:  0.62862915 ,  1257  seconds\n",
      "Loss is:  0.65963346 ,  1323  seconds\n",
      "Loss is:  0.65871096 ,  1389  seconds\n",
      "Loss is:  0.66825247 ,  1455  seconds\n",
      "Loss is:  0.6681283 ,  1521  seconds\n",
      "Loss is:  0.6372764 ,  1587  seconds\n",
      "Loss is:  0.68310976 ,  1653  seconds\n",
      "Loss is:  0.6155688 ,  1720  seconds\n",
      "Loss is:  0.6675343 ,  1786  seconds\n",
      "Loss is:  0.63307 ,  1852  seconds\n",
      "Loss is:  0.6057222 ,  1918  seconds\n",
      "Loss is:  0.68826896 ,  1984  seconds\n",
      "Loss is:  0.6456762 ,  2051  seconds\n",
      "Loss is:  0.6345126 ,  2117  seconds\n",
      "Loss is:  0.65963423 ,  2183  seconds\n",
      "Loss is:  0.69997436 ,  2249  seconds\n",
      "Loss is:  0.68971175 ,  2315  seconds\n",
      "Loss is:  0.6255753 ,  2381  seconds\n",
      "Loss is:  0.6877739 ,  2447  seconds\n",
      "Loss is:  0.63858557 ,  2513  seconds\n",
      "Loss is:  0.6603235 ,  2579  seconds\n",
      "Loss is:  0.6425799 ,  2646  seconds\n",
      "Loss is:  0.6686618 ,  2712  seconds\n",
      "Loss is:  0.6623797 ,  2778  seconds\n",
      "Loss is:  0.62823224 ,  2844  seconds\n",
      "Loss is:  0.6557501 ,  2910  seconds\n",
      "Loss is:  0.6461249 ,  2976  seconds\n",
      "Loss is:  0.61744803 ,  3042  seconds\n",
      "Loss is:  0.64515984 ,  3108  seconds\n",
      "Loss is:  0.6397772 ,  3175  seconds\n",
      "Loss is:  0.6426532 ,  3241  seconds\n",
      "Loss is:  0.63471395 ,  3307  seconds\n",
      "Loss is:  0.57815117 ,  3374  seconds\n",
      "Loss is:  0.663129 ,  3440  seconds\n",
      "Loss is:  0.6374461 ,  3506  seconds\n",
      "Loss is:  0.587473 ,  3572  seconds\n",
      "Loss is:  0.61181736 ,  3639  seconds\n",
      "Loss is:  0.67504734 ,  3705  seconds\n",
      "Loss is:  0.67676735 ,  3771  seconds\n",
      "Loss is:  0.65126413 ,  3837  seconds\n",
      "Loss is:  0.66880864 ,  3903  seconds\n",
      "Loss is:  0.5824151 ,  3969  seconds\n",
      "Loss is:  0.6116538 ,  4036  seconds\n",
      "Loss is:  0.7232649 ,  4102  seconds\n",
      "Loss is:  0.5645586 ,  4169  seconds\n",
      "Loss is:  0.5420674 ,  4235  seconds\n",
      "Loss is:  0.6311563 ,  4301  seconds\n",
      "Loss is:  0.6141107 ,  4367  seconds\n",
      "Loss is:  0.6319287 ,  4434  seconds\n",
      "Loss is:  0.736804 ,  4500  seconds\n",
      "Loss is:  0.5766342 ,  4566  seconds\n",
      "Loss is:  0.64162475 ,  4632  seconds\n",
      "Loss is:  0.5498758 ,  4699  seconds\n",
      "Loss is:  0.5585263 ,  4765  seconds\n",
      "Loss is:  0.440228 ,  4831  seconds\n",
      "Loss is:  0.5798267 ,  4897  seconds\n",
      "Loss is:  0.6102095 ,  4964  seconds\n",
      "Loss is:  0.556661 ,  5030  seconds\n",
      "Loss is:  0.62832457 ,  5096  seconds\n",
      "Loss is:  0.5074833 ,  5162  seconds\n",
      "Loss is:  0.5469367 ,  5228  seconds\n",
      "Loss is:  0.50792885 ,  5294  seconds\n",
      "Loss is:  0.4820253 ,  5360  seconds\n",
      "Loss is:  0.6613559 ,  5427  seconds\n",
      "Loss is:  0.589551 ,  5493  seconds\n",
      "Loss is:  0.5073463 ,  5559  seconds\n",
      "Loss is:  0.4978358 ,  5625  seconds\n",
      "Loss is:  0.5192463 ,  5691  seconds\n",
      "Loss is:  0.5650456 ,  5757  seconds\n",
      "Loss is:  0.48216596 ,  5823  seconds\n",
      "Loss is:  0.55113554 ,  5889  seconds\n",
      "Loss is:  0.51282567 ,  5955  seconds\n",
      "Loss is:  0.5253864 ,  6021  seconds\n",
      "Loss is:  0.5052909 ,  6088  seconds\n",
      "Loss is:  0.45568123 ,  6154  seconds\n",
      "Loss is:  0.463263 ,  6220  seconds\n",
      "Loss is:  0.59832025 ,  6286  seconds\n",
      "Loss is:  0.48209462 ,  6352  seconds\n",
      "Loss is:  0.6409077 ,  6418  seconds\n",
      "Loss is:  0.5353675 ,  6484  seconds\n",
      "Loss is:  0.50257313 ,  6550  seconds\n",
      "Loss is:  0.4050367 ,  6616  seconds\n",
      "Loss is:  0.40606523 ,  6682  seconds\n",
      "Loss is:  0.49970484 ,  6748  seconds\n",
      "Loss is:  0.45595387 ,  6815  seconds\n",
      "Loss is:  0.39701307 ,  6881  seconds\n",
      "Loss is:  0.4306551 ,  6947  seconds\n",
      "Loss is:  0.41897348 ,  7013  seconds\n",
      "Loss is:  0.44803843 ,  7080  seconds\n",
      "Loss is:  0.3938602 ,  7146  seconds\n",
      "Loss is:  0.481876 ,  7212  seconds\n",
      "Loss is:  0.43255714 ,  7278  seconds\n",
      "Loss is:  0.3887023 ,  7344  seconds\n",
      "Loss is:  0.24620394 ,  7410  seconds\n",
      "Loss is:  0.27364543 ,  7476  seconds\n",
      "Loss is:  0.38695538 ,  7542  seconds\n",
      "Loss is:  0.29913595 ,  7608  seconds\n",
      "Loss is:  0.2851552 ,  7674  seconds\n",
      "Loss is:  0.35068896 ,  7740  seconds\n",
      "Loss is:  0.28638747 ,  7807  seconds\n",
      "Loss is:  0.3445181 ,  7873  seconds\n",
      "Loss is:  0.4511033 ,  7939  seconds\n",
      "Loss is:  0.9458504 ,  8005  seconds\n",
      "Loss is:  0.39513287 ,  8071  seconds\n",
      "Loss is:  0.34952512 ,  8137  seconds\n",
      "Loss is:  0.4491104 ,  8203  seconds\n",
      "Loss is:  0.26091304 ,  8269  seconds\n",
      "Loss is:  0.44869733 ,  8335  seconds\n",
      "Loss is:  0.79294926 ,  8402  seconds\n",
      "Loss is:  0.73690957 ,  8468  seconds\n",
      "Loss is:  0.54869336 ,  8534  seconds\n",
      "Loss is:  0.2694961 ,  8601  seconds\n",
      "Loss is:  0.42606196 ,  8667  seconds\n",
      "Loss is:  0.38574243 ,  8733  seconds\n",
      "Loss is:  0.22278969 ,  8799  seconds\n",
      "Loss is:  0.75764674 ,  8865  seconds\n",
      "Loss is:  0.24405675 ,  8931  seconds\n",
      "Loss is:  0.20780791 ,  8997  seconds\n",
      "Loss is:  0.23021209 ,  9064  seconds\n",
      "Loss is:  0.4467032 ,  9130  seconds\n",
      "Loss is:  0.30388847 ,  9196  seconds\n",
      "Loss is:  0.22310679 ,  9262  seconds\n",
      "Loss is:  0.12214192 ,  9328  seconds\n",
      "Loss is:  0.41883636 ,  9394  seconds\n",
      "Loss is:  0.33035928 ,  9460  seconds\n",
      "Loss is:  0.42688146 ,  9526  seconds\n",
      "Loss is:  0.39011288 ,  9592  seconds\n",
      "Loss is:  0.27073196 ,  9658  seconds\n",
      "Loss is:  0.59953827 ,  9724  seconds\n",
      "Loss is:  0.21709321 ,  9790  seconds\n",
      "Loss is:  0.21057 ,  9856  seconds\n",
      "Loss is:  0.49235582 ,  9922  seconds\n",
      "Loss is:  0.25045845 ,  9988  seconds\n",
      "Loss is:  0.30498898 ,  10054  seconds\n",
      "Loss is:  0.21995418 ,  10120  seconds\n",
      "Loss is:  0.42116675 ,  10186  seconds\n",
      "Loss is:  0.25190935 ,  10253  seconds\n",
      "Loss is:  0.29412767 ,  10319  seconds\n",
      "Loss is:  0.22281142 ,  10385  seconds\n",
      "Loss is:  0.10847447 ,  10451  seconds\n",
      "Loss is:  0.39648545 ,  10517  seconds\n",
      "Loss is:  0.32296503 ,  10583  seconds\n",
      "Loss is:  0.17108048 ,  10650  seconds\n",
      "Loss is:  0.29161036 ,  10716  seconds\n",
      "Loss is:  0.14047255 ,  10782  seconds\n",
      "Loss is:  0.20619094 ,  10848  seconds\n",
      "Loss is:  0.24786413 ,  10914  seconds\n",
      "Loss is:  0.18446611 ,  10980  seconds\n",
      "Loss is:  0.29167506 ,  11046  seconds\n",
      "Loss is:  0.16702795 ,  11112  seconds\n",
      "Loss is:  0.23674719 ,  11178  seconds\n",
      "Loss is:  0.4094483 ,  11244  seconds\n",
      "Loss is:  0.42628828 ,  11310  seconds\n",
      "Loss is:  0.34732452 ,  11376  seconds\n",
      "Loss is:  0.23567583 ,  11442  seconds\n",
      "Loss is:  0.41892517 ,  11509  seconds\n",
      "Loss is:  0.33264405 ,  11575  seconds\n",
      "Loss is:  0.16043079 ,  11641  seconds\n",
      "Loss is:  0.23057784 ,  11707  seconds\n",
      "Loss is:  0.48536932 ,  11773  seconds\n",
      "Loss is:  0.35344586 ,  11839  seconds\n",
      "Loss is:  0.3004064 ,  11905  seconds\n",
      "Loss is:  0.3099138 ,  11971  seconds\n",
      "Loss is:  0.16318803 ,  12037  seconds\n",
      "Loss is:  0.23379707 ,  12104  seconds\n",
      "Loss is:  0.16230793 ,  12170  seconds\n",
      "Loss is:  0.34416285 ,  12236  seconds\n",
      "Loss is:  0.5777389 ,  12302  seconds\n",
      "Loss is:  0.31434476 ,  12368  seconds\n",
      "Loss is:  0.27859473 ,  12435  seconds\n",
      "Loss is:  0.25430867 ,  12501  seconds\n",
      "Loss is:  0.18870415 ,  12568  seconds\n",
      "Loss is:  0.2521144 ,  12634  seconds\n",
      "Loss is:  0.2334434 ,  12701  seconds\n",
      "Loss is:  0.24268343 ,  12768  seconds\n",
      "Loss is:  0.09958541 ,  12834  seconds\n",
      "Loss is:  0.24625637 ,  12901  seconds\n",
      "Loss is:  0.13011403 ,  12967  seconds\n",
      "Loss is:  0.09753474 ,  13033  seconds\n",
      "Loss is:  0.4092315 ,  13100  seconds\n",
      "Loss is:  0.1639961 ,  13167  seconds\n",
      "Loss is:  0.22439766 ,  13234  seconds\n",
      "Loss is:  0.34404707 ,  13300  seconds\n",
      "Loss is:  0.49106488 ,  13367  seconds\n",
      "Loss is:  0.09602072 ,  13434  seconds\n",
      "Loss is:  0.19376071 ,  13500  seconds\n",
      "Loss is:  0.25207478 ,  13567  seconds\n",
      "Loss is:  0.20492049 ,  13634  seconds\n",
      "Loss is:  0.2837927 ,  13700  seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.26807606 ,  13767  seconds\n",
      "Loss is:  0.15837936 ,  13833  seconds\n",
      "Loss is:  0.09417925 ,  13899  seconds\n",
      "Loss is:  0.2542978 ,  13966  seconds\n",
      "Loss is:  0.1962748 ,  14032  seconds\n",
      "Loss is:  0.2528983 ,  14098  seconds\n",
      "Loss is:  0.23969913 ,  14164  seconds\n",
      "Loss is:  0.2147023 ,  14230  seconds\n",
      "Loss is:  0.19946782 ,  14296  seconds\n",
      "Loss is:  0.14208232 ,  14362  seconds\n",
      "Loss is:  0.22451814 ,  14428  seconds\n",
      "Loss is:  0.37149408 ,  14494  seconds\n",
      "Loss is:  0.24153613 ,  14560  seconds\n",
      "Loss is:  0.078781456 ,  14627  seconds\n",
      "Loss is:  0.10087118 ,  14693  seconds\n",
      "Loss is:  0.2157792 ,  14759  seconds\n",
      "Loss is:  0.37425718 ,  14825  seconds\n",
      "Loss is:  0.1001982 ,  14891  seconds\n",
      "Loss is:  0.18356828 ,  14958  seconds\n",
      "Loss is:  0.28114736 ,  15024  seconds\n",
      "Loss is:  0.25548646 ,  15090  seconds\n",
      "Loss is:  0.26080656 ,  15157  seconds\n",
      "Loss is:  0.3924642 ,  15223  seconds\n",
      "Loss is:  0.27326497 ,  15289  seconds\n",
      "Loss is:  0.3881944 ,  15355  seconds\n",
      "Loss is:  0.2668226 ,  15421  seconds\n",
      "Loss is:  0.26169538 ,  15488  seconds\n",
      "Loss is:  0.33856297 ,  15554  seconds\n",
      "Loss is:  0.16899687 ,  15620  seconds\n",
      "Loss is:  0.18645395 ,  15686  seconds\n",
      "Loss is:  0.5683999 ,  15752  seconds\n",
      "Loss is:  0.19093788 ,  15818  seconds\n",
      "Loss is:  0.2914182 ,  15885  seconds\n",
      "Loss is:  0.13630763 ,  15951  seconds\n",
      "Loss is:  0.5393058 ,  16017  seconds\n",
      "Loss is:  0.10224334 ,  16083  seconds\n",
      "Loss is:  0.10978544 ,  16148  seconds\n",
      "Loss is:  0.21588193 ,  16215  seconds\n",
      "Loss is:  0.117607355 ,  16280  seconds\n",
      "Loss is:  0.07510255 ,  16347  seconds\n",
      "Loss is:  0.36636326 ,  16413  seconds\n",
      "Loss is:  0.08796691 ,  16479  seconds\n",
      "Loss is:  0.043462772 ,  16545  seconds\n",
      "Loss is:  0.12178725 ,  16611  seconds\n",
      "Loss is:  0.4890459 ,  16677  seconds\n",
      "Loss is:  0.24470037 ,  16743  seconds\n",
      "Loss is:  0.13104755 ,  16809  seconds\n",
      "Loss is:  0.10157716 ,  16875  seconds\n",
      "Loss is:  0.17809983 ,  16941  seconds\n",
      "Loss is:  0.45084533 ,  17007  seconds\n",
      "Loss is:  0.16830076 ,  17073  seconds\n",
      "Loss is:  0.34128413 ,  17139  seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    # Create the graph\n",
    "    sentimentGraph = SentimentGraph()\n",
    "    sentimentGraph.CreateGraph()\n",
    "\n",
    "    # Initialize the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train model\n",
    "    TrainModel(sess, sentimentGraph)\n",
    "    \n",
    "    # Save the model variables\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, pathToCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/binaryclassifier/model.ckpt\n",
      "Testing Results:\n",
      "The average accuracy is:  0.7895833\n",
      "The average loss is:  0.69423616\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Create the graph\n",
    "    sentimentGraph = SentimentGraph()\n",
    "    sentimentGraph.CreateGraph()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, pathToCheckpoint)\n",
    "\n",
    "    # Test model\n",
    "    TestModel(sess, sentimentGraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
